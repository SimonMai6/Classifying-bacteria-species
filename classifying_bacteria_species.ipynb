{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i2nBMaIurk11",
        "outputId": "18953cbd-f06a-43e8-9a3b-8dfa9aaadd82"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\newusername\\AppData\\Roaming\\Python\\Python37\\site-packages\\pandas\\compat\\_optional.py:138: UserWarning: Pandas requires version '2.7.0' or newer of 'numexpr' (version '2.6.9' currently installed).\n",
            "  warnings.warn(msg, UserWarning)\n"
          ]
        }
      ],
      "source": [
        "# Importing libraries and packages:\n",
        "\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xUDNx_aGpKKe"
      },
      "outputs": [],
      "source": [
        "df_train  = pd.read_csv(\"tabular-playground-series-feb-2022/train.csv\", dtype={'target': 'category'}, index_col='row_id')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Cusu4lerk2P",
        "outputId": "764ef690-d707-450a-ac7b-668216cc738d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>row_id</th>\n",
              "      <th>A0T0G0C10</th>\n",
              "      <th>A0T0G1C9</th>\n",
              "      <th>A0T0G2C8</th>\n",
              "      <th>A0T0G3C7</th>\n",
              "      <th>A0T0G4C6</th>\n",
              "      <th>A0T0G5C5</th>\n",
              "      <th>A0T0G6C4</th>\n",
              "      <th>A0T0G7C3</th>\n",
              "      <th>A0T0G8C2</th>\n",
              "      <th>...</th>\n",
              "      <th>A8T0G1C1</th>\n",
              "      <th>A8T0G2C0</th>\n",
              "      <th>A8T1G0C1</th>\n",
              "      <th>A8T1G1C0</th>\n",
              "      <th>A8T2G0C0</th>\n",
              "      <th>A9T0G0C1</th>\n",
              "      <th>A9T0G1C0</th>\n",
              "      <th>A9T1G0C0</th>\n",
              "      <th>A10T0G0C0</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>Streptococcus_pyogenes</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>Salmonella_enterica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>1.046326e-06</td>\n",
              "      <td>Salmonella_enterica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>4.632568e-08</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>Salmonella_enterica</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>Enterococcus_hirae</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 288 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   row_id     A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  \\\n",
              "0       0 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240   \n",
              "1       1 -9.536743e-07 -0.000010 -0.000043  0.000886 -0.000200  0.000760   \n",
              "2       2 -9.536743e-07 -0.000002  0.000007  0.000129  0.000268  0.000270   \n",
              "3       3  4.632568e-08 -0.000006  0.000012  0.000245  0.000492  0.000522   \n",
              "4       4 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240   \n",
              "\n",
              "   A0T0G6C4  A0T0G7C3  A0T0G8C2  ...  A8T0G1C1  A8T0G2C0  A8T1G0C1  A8T1G1C0  \\\n",
              "0 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043 -0.000086 -0.000086   \n",
              "1 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043  0.000914  0.000914   \n",
              "2  0.000243  0.000125  0.000001  ...  0.000084  0.000048  0.000081  0.000106   \n",
              "3  0.000396  0.000197 -0.000003  ...  0.000151  0.000100  0.000180  0.000202   \n",
              "4 -0.000200 -0.000114 -0.000043  ... -0.000086 -0.000043 -0.000086 -0.000086   \n",
              "\n",
              "   A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0     A10T0G0C0  \\\n",
              "0 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
              "1 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
              "2  0.000072  0.000010  0.000008  0.000019  1.046326e-06   \n",
              "3  0.000153  0.000021  0.000015  0.000046 -9.536743e-07   \n",
              "4 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07   \n",
              "\n",
              "                   target  \n",
              "0  Streptococcus_pyogenes  \n",
              "1     Salmonella_enterica  \n",
              "2     Salmonella_enterica  \n",
              "3     Salmonella_enterica  \n",
              "4      Enterococcus_hirae  \n",
              "\n",
              "[5 rows x 288 columns]"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_train.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CJiMtvfIrk2U",
        "outputId": "674a22c8-284c-4eba-b0e2-98d0cd9fb399"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A0T0G0C10</th>\n",
              "      <th>A0T0G1C9</th>\n",
              "      <th>A0T0G2C8</th>\n",
              "      <th>A0T0G3C7</th>\n",
              "      <th>A0T0G4C6</th>\n",
              "      <th>A0T0G5C5</th>\n",
              "      <th>A0T0G6C4</th>\n",
              "      <th>A0T0G7C3</th>\n",
              "      <th>A0T0G8C2</th>\n",
              "      <th>A0T0G9C1</th>\n",
              "      <th>...</th>\n",
              "      <th>A8T0G0C2</th>\n",
              "      <th>A8T0G1C1</th>\n",
              "      <th>A8T0G2C0</th>\n",
              "      <th>A8T1G0C1</th>\n",
              "      <th>A8T1G1C0</th>\n",
              "      <th>A8T2G0C0</th>\n",
              "      <th>A9T0G0C1</th>\n",
              "      <th>A9T0G1C0</th>\n",
              "      <th>A9T1G0C0</th>\n",
              "      <th>A10T0G0C0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>1.046326e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.632568e-08</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 286 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  A0T0G6C4  \\\n",
              "0 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240 -0.000200   \n",
              "1 -9.536743e-07 -0.000010 -0.000043  0.000886 -0.000200  0.000760 -0.000200   \n",
              "2 -9.536743e-07 -0.000002  0.000007  0.000129  0.000268  0.000270  0.000243   \n",
              "3  4.632568e-08 -0.000006  0.000012  0.000245  0.000492  0.000522  0.000396   \n",
              "4 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240 -0.000200   \n",
              "\n",
              "   A0T0G7C3  A0T0G8C2  A0T0G9C1  ...  A8T0G0C2  A8T0G1C1  A8T0G2C0  A8T1G0C1  \\\n",
              "0 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043 -0.000086   \n",
              "1 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043  0.000914   \n",
              "2  0.000125  0.000001 -0.000007  ...  0.000042  0.000084  0.000048  0.000081   \n",
              "3  0.000197 -0.000003 -0.000007  ...  0.000068  0.000151  0.000100  0.000180   \n",
              "4 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043 -0.000086   \n",
              "\n",
              "   A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0     A10T0G0C0  \n",
              "0 -0.000086 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "1  0.000914 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "2  0.000106  0.000072  0.000010  0.000008  0.000019  1.046326e-06  \n",
              "3  0.000202  0.000153  0.000021  0.000015  0.000046 -9.536743e-07  \n",
              "4 -0.000086 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "\n",
              "[5 rows x 286 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = df_train.drop(columns = ['target','row_id'])\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pumbbj11rk2X",
        "outputId": "7874fe71-1c35-4dd1-95bb-674a4eb9a959"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    9\n",
              "1    6\n",
              "2    6\n",
              "3    6\n",
              "4    2\n",
              "Name: target, dtype: int32"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encoding categorical features\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = pd.DataFrame(le.fit_transform(df_train['target']), columns=['target'])\n",
        "y = y.target\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4PIaRN3rk2b",
        "outputId": "63c8544a-f56f-45ed-c9c8-418c62825b08"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(160000, 286)\n",
            "(160000,)\n",
            "(40000, 286)\n",
            "(40000,)\n"
          ]
        }
      ],
      "source": [
        "# Randomly splitting the original dataset into training set and testing set:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)\n",
        "\n",
        "# print the size of the traning set:\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "# print the size of the testing set:\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dg3O97kWrk2e"
      },
      "source": [
        "# Artificial Neural Networks (ANN) SciKitLearn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "slSjdaVDrk2l"
      },
      "outputs": [],
      "source": [
        "# \"my_ANN\" is instantiated as an \"object\" of MLPClassifier \"class\". \n",
        "# hidden_layer_sizes: The ith element represents the number of neurons in the ith hidden layer.\n",
        "# activation: the non-linear function. 'logistic' means Sigmoid Function!\n",
        "# solver: the type of minimization problem: â€˜sgdâ€™ and 'adam' refer to stochastic gradient descent.\n",
        "# alpha: regularization parameter (L2 penalty term).\n",
        "# learning_rate_init: learning rate.\n",
        "# batch_sizeint: Size of minibatches for stochastic optimizers.\n",
        "# verbosebool: Whether to print progress\n",
        "# tol: Tolerance for the optimization. When the loss/score is not improving by at least tol in consecutive iterations. \n",
        "\n",
        "# 1 Hidden Layer with 3 neurons:\n",
        "my_ANN = MLPClassifier(hidden_layer_sizes=(3,), activation= 'logistic', \n",
        "                       solver='adam', alpha=1e-5, random_state=1, \n",
        "                       learning_rate_init = 0.1, verbose=True, tol=0.0001)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Xllendprk2q",
        "outputId": "5a10acfd-95da-425d-d339-731f6c2fe4e9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.29662931\n",
            "Iteration 2, loss = 0.99242164\n",
            "Iteration 3, loss = 0.89743887\n",
            "Iteration 4, loss = 0.86414650\n",
            "Iteration 5, loss = 0.84874283\n",
            "Iteration 6, loss = 0.83719575\n",
            "Iteration 7, loss = 0.81729461\n",
            "Iteration 8, loss = 0.80267437\n",
            "Iteration 9, loss = 0.79513060\n",
            "Iteration 10, loss = 0.78846234\n",
            "Iteration 11, loss = 0.78296249\n",
            "Iteration 12, loss = 0.78103027\n",
            "Iteration 13, loss = 0.77660992\n",
            "Iteration 14, loss = 0.77394057\n",
            "Iteration 15, loss = 0.77099627\n",
            "Iteration 16, loss = 0.76912021\n",
            "Iteration 17, loss = 0.76692461\n",
            "Iteration 18, loss = 0.76538350\n",
            "Iteration 19, loss = 0.76382696\n",
            "Iteration 20, loss = 0.76159320\n",
            "Iteration 21, loss = 0.76030453\n",
            "Iteration 22, loss = 0.75975415\n",
            "Iteration 23, loss = 0.75755553\n",
            "Iteration 24, loss = 0.75726117\n",
            "Iteration 25, loss = 0.75565508\n",
            "Iteration 26, loss = 0.75649507\n",
            "Iteration 27, loss = 0.75475329\n",
            "Iteration 28, loss = 0.75499802\n",
            "Iteration 29, loss = 0.75444447\n",
            "Iteration 30, loss = 0.75412006\n",
            "Iteration 31, loss = 0.75343451\n",
            "Iteration 32, loss = 0.75403527\n",
            "Iteration 33, loss = 0.75101596\n",
            "Iteration 34, loss = 0.75223181\n",
            "Iteration 35, loss = 0.75290366\n",
            "Iteration 36, loss = 0.75073586\n",
            "Iteration 37, loss = 0.75124002\n",
            "Iteration 38, loss = 0.74945958\n",
            "Iteration 39, loss = 0.75104629\n",
            "Iteration 40, loss = 0.75036462\n",
            "Iteration 41, loss = 0.75024876\n",
            "Iteration 42, loss = 0.75001148\n",
            "Iteration 43, loss = 0.75213604\n",
            "Iteration 44, loss = 0.74978231\n",
            "Iteration 45, loss = 0.74860629\n",
            "Iteration 46, loss = 0.74970185\n",
            "Iteration 47, loss = 0.74861053\n",
            "Iteration 48, loss = 0.74929810\n",
            "Iteration 49, loss = 0.74915827\n",
            "Iteration 50, loss = 0.74818919\n",
            "Iteration 51, loss = 0.75114562\n",
            "Iteration 52, loss = 0.74906915\n",
            "Iteration 53, loss = 0.74840772\n",
            "Iteration 54, loss = 0.74836867\n",
            "Iteration 55, loss = 0.74867018\n",
            "Iteration 56, loss = 0.74884291\n",
            "Iteration 57, loss = 0.74680792\n",
            "Iteration 58, loss = 0.75073742\n",
            "Iteration 59, loss = 0.74748668\n",
            "Iteration 60, loss = 0.74691875\n",
            "Iteration 61, loss = 0.74897156\n",
            "Iteration 62, loss = 0.74742858\n",
            "Iteration 63, loss = 0.74889493\n",
            "Iteration 64, loss = 0.74911977\n",
            "Iteration 65, loss = 0.74838813\n",
            "Iteration 66, loss = 0.74740007\n",
            "Iteration 67, loss = 0.74894447\n",
            "Iteration 68, loss = 0.74752427\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "MLPClassifier(activation='logistic', alpha=1e-05, hidden_layer_sizes=(3,),\n",
              "              learning_rate_init=0.1, random_state=1, verbose=True)"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training ONLY on the training set:\n",
        "my_ANN.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1uRxYWfKrk2u",
        "outputId": "93db9682-eb48-4e10-87cc-fca3067096d7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[array([[-2.67347864e-01, -2.80284743e+01,  3.21484879e+01],\n",
            "       [-8.85461720e+00,  7.99076388e+00, -2.04215941e+01],\n",
            "       [-3.01873778e+01, -5.84291680e+01, -8.89216124e+00],\n",
            "       [-7.17429431e+01, -4.48666147e+01, -2.99317366e+01],\n",
            "       [-9.37148225e+01,  7.27950358e+00, -3.16546371e+01],\n",
            "       [-9.97463260e+01, -1.26019130e+01, -4.70614824e+01],\n",
            "       [-9.58161371e+01, -2.06980165e+01, -3.91542131e+01],\n",
            "       [-6.06163823e+01, -4.15046530e+01, -1.65382634e+01],\n",
            "       [-1.41535519e+01,  7.51443472e+00, -2.63411864e+01],\n",
            "       [-2.67931851e-01,  6.64281922e-01, -5.67564820e+00],\n",
            "       [-1.25930893e-01, -1.33264588e+00,  1.59779512e+00],\n",
            "       [ 5.63015418e+01,  6.85560231e+01, -8.22258320e+01],\n",
            "       [ 1.05188397e+01, -3.40307010e+01, -4.59578912e+01],\n",
            "       [-8.39413175e+01,  1.46464170e+01, -2.73321099e+01],\n",
            "       [-1.89293985e+02, -3.60997939e+01, -1.08624978e+01],\n",
            "       [-1.21872158e+02, -2.23256077e+01, -1.78686067e+01],\n",
            "       [-1.40649312e+02, -2.48244653e+01, -7.96798562e+00],\n",
            "       [-6.56667133e+01, -1.48787739e+01, -1.22904907e+01],\n",
            "       [-1.33447622e+02, -1.09529020e+01, -1.07416212e+01],\n",
            "       [-4.55491164e+01,  5.66337268e+01, -1.54865106e+01],\n",
            "       [-4.81295229e+00,  5.25618292e+00, -1.99541714e+00],\n",
            "       [-9.16801461e+00,  4.14893076e+00,  1.25775333e+01],\n",
            "       [-6.21269025e+00,  6.27610209e+00, -2.03433751e+01],\n",
            "       [-1.09744606e+02, -2.01810167e+01, -2.33055574e+01],\n",
            "       [-1.78673320e+01, -2.13978527e+01, -5.10086789e+00],\n",
            "       [ 2.15410515e+01, -1.61124678e+01, -1.80056629e+00],\n",
            "       [-4.91098437e+01, -1.83852213e+01, -4.58100731e+00],\n",
            "       [-4.71711086e+01, -9.28384153e+00, -7.11836898e+00],\n",
            "       [-7.08868832e+01, -7.43031204e+00, -4.44056174e+00],\n",
            "       [-1.62603537e+01,  1.99169118e+01,  2.86359137e+01],\n",
            "       [ 1.53769912e+01, -2.81646885e+01, -4.36438443e+01],\n",
            "       [-1.70033590e+01, -1.34477817e+01, -1.48312615e+01],\n",
            "       [ 3.40107680e+01, -8.01601135e+00, -6.36657656e+00],\n",
            "       [ 1.95666713e+01, -8.21927083e+00, -3.80477626e+00],\n",
            "       [-1.38277580e+01, -2.03860186e+00, -3.31324613e+00],\n",
            "       [-9.84242205e+00, -6.20043149e+00, -4.06468292e-01],\n",
            "       [-1.24639417e+01, -1.15599343e+00, -2.57200290e+00],\n",
            "       [-4.06429991e+01,  9.59495414e+00,  1.36414122e+01],\n",
            "       [ 5.26480718e+01, -1.48083884e+01, -1.47356395e+01],\n",
            "       [ 4.65871097e+01, -1.58429536e+00, -5.59294983e+00],\n",
            "       [ 3.99759972e+01, -3.84948850e+00, -9.83635893e-01],\n",
            "       [ 3.31797200e+01,  4.36949867e+00, -1.97591186e+00],\n",
            "       [ 1.31488821e+01,  3.56148226e-01,  3.96929207e+00],\n",
            "       [ 2.98644916e+01, -9.62122635e-01, -8.26522211e+00],\n",
            "       [-6.24667700e+01,  1.23690942e+01,  1.37038201e+01],\n",
            "       [ 5.50186911e+01,  1.70322899e+01, -9.24633471e+00],\n",
            "       [ 8.40107995e+01,  6.01876222e+00,  5.97810176e+00],\n",
            "       [ 5.54288914e+01, -4.72927231e+00,  4.68582605e+00],\n",
            "       [ 3.56778090e+01,  5.30260962e+00,  7.65969337e+00],\n",
            "       [ 2.10607487e+01,  1.00855121e+01,  1.40476101e+01],\n",
            "       [ 1.98243809e+01,  2.70659180e+01,  7.40479890e+00],\n",
            "       [ 8.63492310e+01,  6.65981673e+00,  2.14553589e+01],\n",
            "       [ 8.75559544e+01,  7.89795020e+00,  1.46124349e+01],\n",
            "       [ 2.78927473e+01,  2.90089690e+00,  1.10530883e+01],\n",
            "       [ 9.61214662e+00,  5.78660041e+00,  1.03785224e+01],\n",
            "       [ 4.79701699e+01,  1.40834341e+01,  3.14257958e+01],\n",
            "       [ 9.22495240e+01,  1.35847960e+01,  1.84422633e+01],\n",
            "       [ 2.85970349e+01,  2.14511273e+00,  1.27884174e+01],\n",
            "       [ 3.65476951e+01,  7.37360096e+00,  2.37725363e+01],\n",
            "       [-7.82028221e+00,  9.73644065e+00,  2.03819817e+01],\n",
            "       [ 3.09353923e+01,  5.95210443e+00,  2.06979466e+01],\n",
            "       [ 2.27611865e+01, -1.49758397e+00,  2.53746131e+01],\n",
            "       [-4.39173290e+01, -2.05524847e+00,  4.72423252e+01],\n",
            "       [ 4.08555297e+00,  4.31094901e+01,  5.48705369e+01],\n",
            "       [-3.01941635e+01, -8.90121357e+00,  2.50087975e+01],\n",
            "       [-9.19847716e+00, -4.40452104e+00,  2.20244051e+01],\n",
            "       [-3.92527388e+00,  5.97998456e-01, -2.87246306e+00],\n",
            "       [-3.01201850e+01,  8.68017278e+01, -3.52594249e+01],\n",
            "       [-8.69359116e+01, -1.40883938e+01, -2.18809834e+01],\n",
            "       [-1.34564859e+02, -2.91480321e+01, -1.68449158e+01],\n",
            "       [-8.92257423e+01, -2.34522760e+01, -1.64921673e+01],\n",
            "       [-8.15881278e+01, -4.47532968e+01, -1.73004612e+01],\n",
            "       [-1.51533660e+02, -2.52472373e+01, -3.18919646e+01],\n",
            "       [-3.87850804e+01, -7.64932267e+00, -4.95056672e+00],\n",
            "       [ 2.80413721e+00, -3.31124396e+01, -4.09998003e+01],\n",
            "       [-7.17480556e+00,  1.10177398e+01, -5.12405346e+00],\n",
            "       [-3.18455488e+01, -2.54508089e+00,  1.96971734e+01],\n",
            "       [-1.17993212e+02, -6.08863775e+00, -3.16193203e+01],\n",
            "       [-2.47775443e+01, -1.58851746e+01, -1.07389565e+01],\n",
            "       [-2.16293980e+01, -2.57922528e+01, -6.83762720e+00],\n",
            "       [-5.86310391e+01, -2.43447410e+01, -1.36867056e+01],\n",
            "       [-4.73407760e+01, -2.49976728e+01, -5.76817720e+00],\n",
            "       [-7.71559870e+01, -1.11074136e+01, -1.20608476e+01],\n",
            "       [-1.98619265e+01, -1.28105167e+00, -7.37515722e+00],\n",
            "       [-2.45727269e+01,  7.34971147e+00, -1.49176701e+01],\n",
            "       [-6.96621266e+01, -1.26970142e+01,  4.93378355e+00],\n",
            "       [-6.72512676e+00, -3.91498418e+00, -1.30645537e+01],\n",
            "       [ 2.42520717e+01, -1.10555874e+01, -2.21824980e+00],\n",
            "       [-3.06796876e+01, -1.23338105e+01, -2.43315350e+00],\n",
            "       [-2.50608410e+01, -1.01475632e+01, -2.81465986e+00],\n",
            "       [-3.28891823e+01, -9.31922728e+00, -4.02451315e-01],\n",
            "       [ 1.17798248e+01, -1.26245304e+00,  3.27483883e+00],\n",
            "       [-1.41869983e+02, -4.62657570e+00, -4.92879751e+01],\n",
            "       [-1.41126353e+00,  1.96987909e-01, -8.40968542e+00],\n",
            "       [ 1.68789002e+01,  8.10695380e+00, -4.45640851e+00],\n",
            "       [ 1.10603366e+01, -8.31681379e-01, -1.77997343e+00],\n",
            "       [ 2.92553134e+01,  6.38078248e-01,  2.53388191e+00],\n",
            "       [ 2.81125603e+01,  3.77586844e-01,  1.47989398e-01],\n",
            "       [ 1.05002152e+01, -2.36118403e-01, -4.32622671e+00],\n",
            "       [ 5.64978179e+01,  1.90670700e+01, -5.94070891e+00],\n",
            "       [ 6.75383489e+01,  6.06833142e+00, -1.27587279e+01],\n",
            "       [ 3.36020726e+01,  3.66077465e+00,  8.96766616e-01],\n",
            "       [ 2.83968654e+01,  3.94432698e-01,  8.67925192e-01],\n",
            "       [ 1.99741350e+01,  7.79537121e+00,  9.70500675e-01],\n",
            "       [ 2.98857079e+01,  1.37925252e+01,  4.39805134e+00],\n",
            "       [ 1.31689865e+01,  1.61020207e+01,  3.61396341e+00],\n",
            "       [ 6.92719894e+01,  1.04457634e+01,  5.99521227e+00],\n",
            "       [ 4.28097676e+01,  5.38898770e+00,  4.27460332e+00],\n",
            "       [ 3.32163877e+01,  6.33736277e+00,  8.47838839e+00],\n",
            "       [ 1.23313438e+01,  1.07908086e+01,  3.53064683e+00],\n",
            "       [ 6.81471725e+00,  1.63784125e+01,  7.82912098e+00],\n",
            "       [ 6.27089441e+01,  6.89525018e+00,  4.36594872e+00],\n",
            "       [ 2.96844286e+01,  1.02826806e+01,  1.40449266e+01],\n",
            "       [-6.98541329e+00,  6.79682780e+00,  7.76095747e+00],\n",
            "       [-2.96694227e-01,  9.55919726e+00,  4.03380146e+00],\n",
            "       [ 2.74368623e+01,  3.47019624e+00,  2.60461403e+01],\n",
            "       [ 1.18118468e+01, -5.24590931e+00,  1.00984392e+01],\n",
            "       [-3.14392341e+00,  1.74155957e+01,  3.11905771e+01],\n",
            "       [ 9.00399263e+00,  1.04992065e+00,  1.67638482e+01],\n",
            "       [ 1.31234200e+01,  7.81176625e+00,  2.80058642e+01],\n",
            "       [-6.21142435e+01, -4.73343066e+01,  1.24688686e+01],\n",
            "       [-9.64218185e+00, -2.58219142e+01, -1.40353621e+01],\n",
            "       [-9.38535478e+00, -3.51143494e+00, -1.24711028e+01],\n",
            "       [-5.34779039e+01, -1.71156527e+01, -4.94978741e+00],\n",
            "       [-9.45467992e+01, -2.17903038e+01, -4.51839088e+00],\n",
            "       [-4.51662170e+01, -2.36463117e+01, -8.39715369e+00],\n",
            "       [-4.08825014e+01, -2.11262323e+01, -7.45208744e+00],\n",
            "       [-3.03280215e+01, -1.95348025e+01, -2.37443900e+01],\n",
            "       [ 1.67192575e+01, -3.06562548e-01,  2.86405664e+00],\n",
            "       [-3.03974180e+00,  4.56819770e+01,  2.25420184e+01],\n",
            "       [-4.83443217e+01,  2.19090610e+01, -1.50196616e+01],\n",
            "       [-3.02396357e+01, -7.41752927e-01, -2.41185500e+00],\n",
            "       [-1.33202003e+01, -1.02421126e+01, -1.76577634e+00],\n",
            "       [-3.89140492e+00, -1.17768338e+01, -3.16567548e+00],\n",
            "       [-9.36363541e+00, -9.04288454e+00, -3.87989368e+00],\n",
            "       [-1.20251054e+01, -7.97777110e+00, -2.01357928e+00],\n",
            "       [-5.03181196e+00, -1.32068824e+00, -1.44420878e+01],\n",
            "       [-7.38073258e+01,  6.71636372e+00, -1.21545971e+01],\n",
            "       [-3.24270061e+01, -2.41043813e+01, -6.56556437e+00],\n",
            "       [ 1.19440040e+01,  4.61449404e+00, -8.76733229e+00],\n",
            "       [ 1.32008630e+01, -3.18870826e+00, -7.79433219e-01],\n",
            "       [ 1.59410919e+01,  2.44618407e+00, -2.32595844e-01],\n",
            "       [ 1.57095278e+00, -3.09541197e-01, -4.14572628e-01],\n",
            "       [ 6.38380205e+00,  1.40925781e+00, -6.24331082e-01],\n",
            "       [-5.68456728e+00,  1.89443860e+00, -5.83695444e+00],\n",
            "       [ 1.88257379e+01,  9.64835098e-01, -8.95264873e+00],\n",
            "       [ 1.65660041e+01,  8.11131945e+00, -2.48369838e+00],\n",
            "       [ 1.45500783e+01,  4.22542825e+00, -1.91073960e+00],\n",
            "       [ 1.85352236e+00,  4.31187440e+00, -4.26316400e+00],\n",
            "       [ 3.67508592e+00,  8.40740577e+00, -1.65115144e+00],\n",
            "       [ 7.15421430e+00,  9.81236183e+00, -9.03577330e-01],\n",
            "       [ 4.09411321e+01,  9.34546239e+00,  4.40397382e+00],\n",
            "       [ 1.62782203e+01,  8.24823855e+00, -1.70672931e+00],\n",
            "       [ 8.25954668e+00,  8.19280151e+00, -3.50670020e+00],\n",
            "       [-5.17444198e+00,  8.25973447e+00,  1.36451284e+00],\n",
            "       [ 2.96672104e+01,  9.45764493e+00, -4.38139962e+00],\n",
            "       [ 1.57158618e+01,  9.57834893e+00,  3.50765863e+00],\n",
            "       [ 1.83499809e+01,  5.57349931e+00,  7.81938638e+00],\n",
            "       [-9.85211095e-01,  1.08195755e+01,  3.81348444e+00],\n",
            "       [ 6.89229769e+00,  1.14340000e+01,  9.26311743e-01],\n",
            "       [ 3.10134008e-01,  6.90410787e+00,  4.22026181e+00],\n",
            "       [-6.89673136e+00,  9.85368802e-01,  7.36182536e+00],\n",
            "       [-9.19162533e+00,  1.12833167e+01,  4.64687740e+00],\n",
            "       [-2.63058196e+01, -7.98795805e+00,  9.35697957e-01],\n",
            "       [-2.58925798e+01, -6.13700483e+00,  1.06931626e+01],\n",
            "       [-6.79039450e+00, -6.16929895e+00,  1.07017650e+01],\n",
            "       [-9.18820004e+00,  4.39318508e+01, -1.96723681e+01],\n",
            "       [ 8.46603413e+00, -7.66695326e-01, -4.76174118e-01],\n",
            "       [-3.39477746e+01, -2.14382609e+00, -3.10189115e+00],\n",
            "       [-1.61447304e+01, -8.41897762e+00, -5.69904687e-01],\n",
            "       [ 2.22698947e+01, -5.69300103e+00, -2.20440251e+00],\n",
            "       [-3.94697070e+01, -9.66511374e+00, -9.68379567e+00],\n",
            "       [ 5.16589295e+01, -1.51933791e+01, -2.59279801e+00],\n",
            "       [ 4.53832758e+01, -1.18768329e+01, -6.00651912e+00],\n",
            "       [ 6.17512680e+01, -6.32031261e-02, -8.79261003e+00],\n",
            "       [ 1.25372163e+01,  3.65112031e+00,  2.37441716e-01],\n",
            "       [ 1.95281452e+01,  2.78032170e+00, -2.01633964e+00],\n",
            "       [ 6.71846017e+00, -7.09722158e-01, -2.92641543e+00],\n",
            "       [ 2.44535945e+01, -4.26979136e+00, -4.73280121e+00],\n",
            "       [ 1.71595466e+01,  8.04100610e-01, -5.17038654e+00],\n",
            "       [ 6.79253776e+01,  1.86886294e+01, -9.10939087e+00],\n",
            "       [ 2.72058514e+01,  1.21439082e+01, -9.49759997e+00],\n",
            "       [-2.33315412e+00,  3.17345876e+00,  1.07117785e+00],\n",
            "       [ 7.72418320e+00,  6.75099096e+00, -3.28238562e+00],\n",
            "       [ 1.46880055e+01,  6.58696698e+00, -3.20459808e+00],\n",
            "       [ 4.65474770e+00,  7.49010966e+00, -2.95738300e+00],\n",
            "       [ 1.27890073e+01, -1.15372854e+00, -5.88648487e+00],\n",
            "       [ 1.40451071e+01,  4.28711237e+00, -3.33541823e+00],\n",
            "       [ 3.96155335e+00,  5.81424156e+00, -1.58473067e+00],\n",
            "       [-1.76887717e+00,  8.39688181e+00, -1.99968007e+00],\n",
            "       [-3.82395391e+00,  6.18674737e+00, -2.06520831e+00],\n",
            "       [ 2.02284843e+01,  7.92299422e+00, -4.68809353e+00],\n",
            "       [ 1.60054252e+01,  1.03337259e+01,  4.56729616e+00],\n",
            "       [-1.23847033e+00,  1.41412949e+01,  4.76558605e-01],\n",
            "       [-1.04244728e+01,  2.91429432e+00, -2.35980782e+00],\n",
            "       [-1.99231739e+01,  5.02169390e+00, -6.68269154e+00],\n",
            "       [-1.35862367e+01,  9.77961410e+00,  2.44029994e+00],\n",
            "       [-1.38945907e+01,  5.58409476e+00, -3.93975053e+00],\n",
            "       [ 3.73946659e+00,  1.35655232e+01,  3.47976607e+00],\n",
            "       [-2.67069699e+00,  3.91916279e+00,  9.27397376e+00],\n",
            "       [-1.92178523e+01,  5.08857030e+00,  1.07710010e+01],\n",
            "       [-3.80005880e+01, -1.33435065e+01,  9.77212576e+00],\n",
            "       [ 6.27589718e+00,  1.66227017e-01,  1.03208229e+01],\n",
            "       [-3.13447293e+01, -1.19525938e+01,  2.54398486e+00],\n",
            "       [ 1.44843839e+01, -1.97950498e+00,  5.42367258e+00],\n",
            "       [ 3.62112356e+01, -6.72589129e+00,  1.84902162e+00],\n",
            "       [ 4.05373470e+01, -3.82109246e+00,  3.09354361e+00],\n",
            "       [ 5.02868388e+01,  1.81441655e+00, -4.36994354e+00],\n",
            "       [ 7.80980763e+01,  9.22334439e+00, -2.62078665e+00],\n",
            "       [ 3.59273973e+01,  1.02682028e+01,  9.54160416e+00],\n",
            "       [ 8.05532155e-01,  6.30877303e+00, -1.89248850e+00],\n",
            "       [ 1.01620444e+01,  1.31427953e+00, -1.30054546e-01],\n",
            "       [ 2.68851058e+01,  1.50992252e+00,  6.85230563e-01],\n",
            "       [ 4.52846964e+01, -1.34054024e+00,  4.16062920e-01],\n",
            "       [ 3.93251756e+01,  5.33065612e+00,  1.53644859e+00],\n",
            "       [ 1.90511473e+00,  1.42011468e+01, -4.38366563e+00],\n",
            "       [ 3.14665259e+00,  8.94126320e+00,  1.99545195e+00],\n",
            "       [-6.68530308e+00,  4.86721392e+00, -3.20741789e+00],\n",
            "       [ 8.00199712e+00,  6.48242111e+00, -1.75927193e+00],\n",
            "       [ 3.63650503e+01,  1.34993860e+01, -3.29636118e+00],\n",
            "       [-8.36021831e+00,  1.14637832e+01, -5.08875376e+00],\n",
            "       [-1.15513871e+01,  6.88706584e+00, -2.45578768e+00],\n",
            "       [-1.38382648e+01,  5.12935687e+00, -3.72625186e+00],\n",
            "       [ 1.01062059e+01,  1.19779059e+01, -3.39526781e+00],\n",
            "       [-1.77577933e+01,  1.45494621e+01, -9.71353799e-01],\n",
            "       [-2.64121944e+01, -3.67627536e-01, -4.86069822e+00],\n",
            "       [-1.52329940e+01,  1.37902957e+01, -1.81719097e+00],\n",
            "       [-1.68551884e+01,  6.25099976e+00,  4.00815926e+00],\n",
            "       [-2.61735682e+01,  4.63477777e+00,  3.55157365e-01],\n",
            "       [-5.30522974e+01, -1.02302155e+01,  1.05991741e+01],\n",
            "       [-1.94603783e+01,  1.57303384e+01,  9.15217152e+00],\n",
            "       [ 3.96169705e+01,  2.70180445e+01,  1.62590457e+01],\n",
            "       [ 4.07265049e+01, -1.23966524e+00,  1.51186391e+00],\n",
            "       [ 3.40679225e+01,  2.07914669e+00,  1.45590547e+00],\n",
            "       [ 7.50490314e+01, -3.00731429e+00,  6.32793007e+00],\n",
            "       [ 8.88458751e+01,  1.90743468e+01,  7.44995668e+00],\n",
            "       [ 1.21096988e+01,  5.46115745e+00, -6.57104789e+00],\n",
            "       [-2.81625779e+00,  1.03777257e+01,  2.42267804e+00],\n",
            "       [ 1.99552578e+01, -6.36581423e-02,  6.44852555e+00],\n",
            "       [ 4.06237929e+01,  3.24572360e+00,  4.23632294e+00],\n",
            "       [ 3.84295327e+01,  3.32802200e+00, -1.97980739e+00],\n",
            "       [ 9.73408735e+00,  1.04605920e+01, -1.43523794e+00],\n",
            "       [-2.46996820e-01,  4.76463242e+00,  3.93034999e+00],\n",
            "       [ 1.06248836e+01,  2.80296916e+00,  1.91921121e+00],\n",
            "       [ 3.79433649e+01,  2.49238722e+00,  2.44042190e-03],\n",
            "       [-1.87979132e+00,  7.30091823e+00, -7.47686511e-02],\n",
            "       [-1.77789846e+00,  7.91511480e+00,  3.65588860e+00],\n",
            "       [-5.02494317e+00,  5.65081614e+00, -7.95086200e-01],\n",
            "       [-1.41566346e+01, -9.81712766e-01, -4.40088017e+00],\n",
            "       [-2.92253642e+01,  7.44332897e-01, -6.24738614e+00],\n",
            "       [-1.59763326e+00, -1.95730392e+01, -6.63472625e+00],\n",
            "       [-6.06362611e+00,  3.32264963e+01,  1.63213098e+01],\n",
            "       [ 3.24099395e+01,  3.76735442e+00,  1.66014841e+01],\n",
            "       [ 3.78241598e+01,  3.49470610e+00,  1.33092360e+01],\n",
            "       [ 5.33851536e+01,  6.10672856e-02,  8.51533679e+00],\n",
            "       [ 8.03028082e+01,  1.14408693e+00, -1.16980715e+01],\n",
            "       [-3.26290159e+00,  1.85132604e+00,  5.50361693e+00],\n",
            "       [ 5.57705181e+00,  5.67568768e+00,  7.19100161e+00],\n",
            "       [ 2.88672635e+01,  4.34179372e+00,  1.58742225e+01],\n",
            "       [ 6.59430925e+01,  6.25017554e+00,  7.50213180e+00],\n",
            "       [-1.59532505e+01,  2.14278137e+00,  1.69944692e+00],\n",
            "       [-9.24897545e+00,  7.51451985e-01,  2.95918004e+00],\n",
            "       [ 1.25005645e+01,  2.42673680e+00,  1.05794744e+01],\n",
            "       [-1.23990549e+00,  8.57883944e+00,  1.02366119e+01],\n",
            "       [ 6.67272668e+00,  4.08261471e+00,  1.23419827e+01],\n",
            "       [-4.94571915e+01, -4.53856040e+00,  5.65180839e+00],\n",
            "       [-4.46688769e+01,  9.12973960e+00,  1.29410120e+01],\n",
            "       [ 3.28357050e+01,  1.71836832e+01,  2.87389164e+01],\n",
            "       [ 4.30138972e+01,  1.14615587e+01,  1.65836609e+01],\n",
            "       [ 6.71779482e+01,  1.99487289e+01,  1.82609367e+01],\n",
            "       [-1.33654813e+01,  3.92233210e+00,  1.66283368e+01],\n",
            "       [ 2.66865330e+01,  1.59798217e+00,  2.15620345e+01],\n",
            "       [ 2.59763681e+01,  1.78279850e+00,  1.76845205e+01],\n",
            "       [ 4.74626831e+00,  7.81704698e+00,  1.51115858e+01],\n",
            "       [-1.09229057e+01, -7.03870518e+00,  8.54335811e+00],\n",
            "       [-3.33886580e+01, -4.21679339e+00,  6.64545985e+00],\n",
            "       [-5.66767055e+01, -2.31136499e+01, -2.93742675e+00],\n",
            "       [ 4.43325978e+01,  1.53336787e+01,  5.04895760e+01],\n",
            "       [-4.75291424e+00, -9.96880327e+00,  2.02542885e+01],\n",
            "       [ 2.89155788e+01, -6.91681510e+00,  2.36570386e+01],\n",
            "       [ 4.03362584e+01, -1.05765521e+01,  2.33767950e+01],\n",
            "       [-9.41732741e+00, -4.52313012e-01,  3.15321901e+01],\n",
            "       [-4.57708649e+01, -3.85854259e+01, -1.03916028e+01],\n",
            "       [ 3.61997525e+01,  1.72611232e+01,  4.95446164e+01],\n",
            "       [ 1.31161536e+01, -8.49218424e+00,  1.42836459e+01],\n",
            "       [-4.98977113e-01, -2.44915814e+00,  8.22665508e+00]]), array([[  13.52119691,  -33.30613753,   -9.7360297 ,   15.58421654,\n",
            "          13.66901418,   12.07314132,   14.93654283,  -37.10943095,\n",
            "           1.71732688,  -11.48001137],\n",
            "       [  -0.92843374,    3.22334339,   10.11560226, -104.87598593,\n",
            "         -76.88581555, -112.61834059, -130.0515396 ,   33.17280341,\n",
            "          33.77528838,   34.917172  ],\n",
            "       [   1.78068043,   38.47183034,   26.15617617,   -4.33470955,\n",
            "          16.09800337, -138.00369922,  -55.52053796,    7.60893645,\n",
            "         -15.38077075,   -0.21700607]])]\n",
            "\n",
            "\n",
            "[array([-2.01926778, -1.66385452, -1.6372457 ]), array([ -7.38013927, -10.45362512, -11.78852041,  11.892779  ,\n",
            "         4.7349273 ,  32.27043365,  22.90313689,  -9.98831125,\n",
            "       -10.25389415, -11.69419281])]\n"
          ]
        }
      ],
      "source": [
        "# Weights:\n",
        "print(my_ANN.coefs_)\n",
        "# The ith element in the list represents the weight matrix corresponding to layer i.\n",
        "\n",
        "print('\\n')\n",
        "\n",
        "# Bias weights:\n",
        "print(my_ANN.intercepts_)\n",
        "# The ith element in the list represents the bias vector corresponding to layer i + 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oD6868kFrk2w",
        "outputId": "f2321386-0865-4329-f9da-d211fdeb3358"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7 0 6 ... 2 4 7]\n"
          ]
        }
      ],
      "source": [
        "# Testing on the testing set:\n",
        "y_predict_ann = my_ANN.predict(X_test)\n",
        "print(y_predict_ann)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WaPPPzqxrk2y",
        "outputId": "1d1c31a1-10b7-4b18-8f92-5710bbced093"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            " accuracy:  0.77115\n"
          ]
        }
      ],
      "source": [
        "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
        "\n",
        "score_ann = accuracy_score(y_test, y_predict_ann)\n",
        "print('\\n','accuracy: ',score_ann)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZXO6NMzZrk20",
        "outputId": "1cc6cc90-9fde-4ffa-aeca-a92f0a416147"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7 0 6 ... 2 4 7] \n",
            "\n",
            "[[4.34067723e-02 2.91809774e-01 1.18670776e-01 ... 3.60686566e-01\n",
            "  4.08757255e-02 9.85535470e-02]\n",
            " [3.38162902e-01 3.02223609e-02 1.10962348e-01 ... 2.97554880e-02\n",
            "  1.22249225e-01 8.69174057e-02]\n",
            " [1.64091310e-07 4.64693742e-07 5.91078668e-08 ... 8.48584417e-08\n",
            "  7.14218921e-09 9.98507454e-09]\n",
            " ...\n",
            " [1.73035091e-01 6.76333762e-02 3.97762757e-01 ... 3.91966889e-02\n",
            "  1.22178158e-01 1.98855387e-01]\n",
            " [1.35624403e-03 3.23004845e-03 1.31951234e-03 ... 1.56599232e-04\n",
            "  3.76404770e-05 7.72288955e-05]\n",
            " [3.92864455e-13 2.26876381e-02 4.94351523e-04 ... 9.71976307e-01\n",
            "  3.02293754e-07 4.84140083e-03]]\n"
          ]
        }
      ],
      "source": [
        "# Estimating the probability (likelihood) of Each Label: \n",
        "y_predict_prob_ann = my_ANN.predict_proba(X_test)\n",
        "print(y_predict_ann,'\\n')\n",
        "print(y_predict_prob_ann)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GxYPLiThrk21",
        "outputId": "fdd66d00-e38e-4707-d60b-75b60841500c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         ... 0.99983324 0.99988883 1.        ]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[0.00000000e+00 2.48756219e-04 7.46268657e-04 ... 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict_prob_ann[:,1], pos_label=1)\n",
        "\n",
        "print(fpr)\n",
        "print(\"\\n\\n\\n\")\n",
        "print(tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ovgSAx9Ork23",
        "outputId": "94edfe9f-19da-4931-a481-4ec53c148674"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9907654888426127\n"
          ]
        }
      ],
      "source": [
        "# AUC:\n",
        "AUC = metrics.auc(fpr, tpr)\n",
        "print(AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qHOglhcwrk24",
        "outputId": "532e6c02-cf26-486f-ed5f-3736a374ceb1"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The following line will tell Jupyter Notebook to keep the figures inside the explorer page \n",
        "# rather than openng a new figure window:\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Roc Curve:\n",
        "plt.plot(fpr, tpr, color='red', lw=2, \n",
        "         label='ROC Curve (area = %0.2f)' % AUC)\n",
        "\n",
        "# Random Guess line:\n",
        "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
        "\n",
        "# Defining The Range of X-Axis and Y-Axis:\n",
        "plt.xlim([-0.005, 1.005])\n",
        "plt.ylim([0.0, 1.01])\n",
        "\n",
        "# Labels, Title, Legend:\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7GK16Fark26"
      },
      "source": [
        "# Cross-Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fx-Abfqzrk27",
        "outputId": "81a8cead-8b80-47f1-977e-38bec4138d42"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>A0T0G0C10</th>\n",
              "      <th>A0T0G1C9</th>\n",
              "      <th>A0T0G2C8</th>\n",
              "      <th>A0T0G3C7</th>\n",
              "      <th>A0T0G4C6</th>\n",
              "      <th>A0T0G5C5</th>\n",
              "      <th>A0T0G6C4</th>\n",
              "      <th>A0T0G7C3</th>\n",
              "      <th>A0T0G8C2</th>\n",
              "      <th>A0T0G9C1</th>\n",
              "      <th>...</th>\n",
              "      <th>A8T0G0C2</th>\n",
              "      <th>A8T0G1C1</th>\n",
              "      <th>A8T0G2C0</th>\n",
              "      <th>A8T1G0C1</th>\n",
              "      <th>A8T1G1C0</th>\n",
              "      <th>A8T2G0C0</th>\n",
              "      <th>A9T0G0C1</th>\n",
              "      <th>A9T0G1C0</th>\n",
              "      <th>A9T1G0C0</th>\n",
              "      <th>A10T0G0C0</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000886</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>0.000760</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>0.000914</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000002</td>\n",
              "      <td>0.000007</td>\n",
              "      <td>0.000129</td>\n",
              "      <td>0.000268</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000125</td>\n",
              "      <td>0.000001</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000042</td>\n",
              "      <td>0.000084</td>\n",
              "      <td>0.000048</td>\n",
              "      <td>0.000081</td>\n",
              "      <td>0.000106</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>0.000008</td>\n",
              "      <td>0.000019</td>\n",
              "      <td>1.046326e-06</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.632568e-08</td>\n",
              "      <td>-0.000006</td>\n",
              "      <td>0.000012</td>\n",
              "      <td>0.000245</td>\n",
              "      <td>0.000492</td>\n",
              "      <td>0.000522</td>\n",
              "      <td>0.000396</td>\n",
              "      <td>0.000197</td>\n",
              "      <td>-0.000003</td>\n",
              "      <td>-0.000007</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000068</td>\n",
              "      <td>0.000151</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>0.000180</td>\n",
              "      <td>0.000202</td>\n",
              "      <td>0.000153</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000046</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-9.536743e-07</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000200</td>\n",
              "      <td>-0.000114</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-9.536743e-07</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 286 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      A0T0G0C10  A0T0G1C9  A0T0G2C8  A0T0G3C7  A0T0G4C6  A0T0G5C5  A0T0G6C4  \\\n",
              "0 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240 -0.000200   \n",
              "1 -9.536743e-07 -0.000010 -0.000043  0.000886 -0.000200  0.000760 -0.000200   \n",
              "2 -9.536743e-07 -0.000002  0.000007  0.000129  0.000268  0.000270  0.000243   \n",
              "3  4.632568e-08 -0.000006  0.000012  0.000245  0.000492  0.000522  0.000396   \n",
              "4 -9.536743e-07 -0.000010 -0.000043 -0.000114 -0.000200 -0.000240 -0.000200   \n",
              "\n",
              "   A0T0G7C3  A0T0G8C2  A0T0G9C1  ...  A8T0G0C2  A8T0G1C1  A8T0G2C0  A8T1G0C1  \\\n",
              "0 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043 -0.000086   \n",
              "1 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043  0.000914   \n",
              "2  0.000125  0.000001 -0.000007  ...  0.000042  0.000084  0.000048  0.000081   \n",
              "3  0.000197 -0.000003 -0.000007  ...  0.000068  0.000151  0.000100  0.000180   \n",
              "4 -0.000114 -0.000043 -0.000010  ... -0.000043 -0.000086 -0.000043 -0.000086   \n",
              "\n",
              "   A8T1G1C0  A8T2G0C0  A9T0G0C1  A9T0G1C0  A9T1G0C0     A10T0G0C0  \n",
              "0 -0.000086 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "1  0.000914 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "2  0.000106  0.000072  0.000010  0.000008  0.000019  1.046326e-06  \n",
              "3  0.000202  0.000153  0.000021  0.000015  0.000046 -9.536743e-07  \n",
              "4 -0.000086 -0.000043 -0.000010 -0.000010 -0.000010 -9.536743e-07  \n",
              "\n",
              "[5 rows x 286 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = df_train.drop(columns = ['target','row_id'])\n",
        "X.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohyvUFBlrk28",
        "outputId": "9eff786f-e9c3-4b7d-8048-5438febff850"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    9\n",
              "1    6\n",
              "2    6\n",
              "3    6\n",
              "4    2\n",
              "Name: target, dtype: int32"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Encoding categorical features\n",
        "le = LabelEncoder()\n",
        "\n",
        "y = pd.DataFrame(le.fit_transform(df_train['target']), columns=['target'])\n",
        "y = y.target\n",
        "y.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kx1nIhGIrk2-",
        "outputId": "80a4ca94-c918-4615-b2e0-bd20f7605bd8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 1, loss = 1.23456532\n",
            "Iteration 2, loss = 0.89356837\n",
            "Iteration 3, loss = 0.83330208\n",
            "Iteration 4, loss = 0.80706129\n",
            "Iteration 5, loss = 0.79320241\n",
            "Iteration 6, loss = 0.78525173\n",
            "Iteration 7, loss = 0.77905436\n",
            "Iteration 8, loss = 0.77472594\n",
            "Iteration 9, loss = 0.77229970\n",
            "Iteration 10, loss = 0.76941051\n",
            "Iteration 11, loss = 0.76521326\n",
            "Iteration 12, loss = 0.76411527\n",
            "Iteration 13, loss = 0.76119727\n",
            "Iteration 14, loss = 0.76015532\n",
            "Iteration 15, loss = 0.75761657\n",
            "Iteration 16, loss = 0.75719090\n",
            "Iteration 17, loss = 0.75618759\n",
            "Iteration 18, loss = 0.75579349\n",
            "Iteration 19, loss = 0.75369965\n",
            "Iteration 20, loss = 0.75327675\n",
            "Iteration 21, loss = 0.75377125\n",
            "Iteration 22, loss = 0.75111344\n",
            "Iteration 23, loss = 0.75256665\n",
            "Iteration 24, loss = 0.75066181\n",
            "Iteration 25, loss = 0.75011642\n",
            "Iteration 26, loss = 0.74927622\n",
            "Iteration 27, loss = 0.74663985\n",
            "Iteration 28, loss = 0.74794803\n",
            "Iteration 29, loss = 0.74745765\n",
            "Iteration 30, loss = 0.74539360\n",
            "Iteration 31, loss = 0.74596625\n",
            "Iteration 32, loss = 0.74515417\n",
            "Iteration 33, loss = 0.74325990\n",
            "Iteration 34, loss = 0.74543419\n",
            "Iteration 35, loss = 0.74336929\n",
            "Iteration 36, loss = 0.74455274\n",
            "Iteration 37, loss = 0.74196604\n",
            "Iteration 38, loss = 0.74195696\n",
            "Iteration 39, loss = 0.74230205\n",
            "Iteration 40, loss = 0.74197488\n",
            "Iteration 41, loss = 0.74208960\n",
            "Iteration 42, loss = 0.74202010\n",
            "Iteration 43, loss = 0.74117570\n",
            "Iteration 44, loss = 0.73983470\n",
            "Iteration 45, loss = 0.74110444\n",
            "Iteration 46, loss = 0.73999115\n",
            "Iteration 47, loss = 0.73971961\n",
            "Iteration 48, loss = 0.73987572\n",
            "Iteration 49, loss = 0.73995295\n",
            "Iteration 50, loss = 0.73935005\n",
            "Iteration 51, loss = 0.74032271\n",
            "Iteration 52, loss = 0.73721998\n",
            "Iteration 53, loss = 0.73863208\n",
            "Iteration 54, loss = 0.73991043\n",
            "Iteration 55, loss = 0.73821387\n",
            "Iteration 56, loss = 0.73946828\n",
            "Iteration 57, loss = 0.73889936\n",
            "Iteration 58, loss = 0.73697906\n",
            "Iteration 59, loss = 0.73798538\n",
            "Iteration 60, loss = 0.73886088\n",
            "Iteration 61, loss = 0.73798187\n",
            "Iteration 62, loss = 0.73826199\n",
            "Iteration 63, loss = 0.73536586\n",
            "Iteration 64, loss = 0.73801613\n",
            "Iteration 65, loss = 0.73666614\n",
            "Iteration 66, loss = 0.73621739\n",
            "Iteration 67, loss = 0.73775499\n",
            "Iteration 68, loss = 0.73574079\n",
            "Iteration 69, loss = 0.73670427\n",
            "Iteration 70, loss = 0.73697169\n",
            "Iteration 71, loss = 0.73562875\n",
            "Iteration 72, loss = 0.73596390\n",
            "Iteration 73, loss = 0.73605071\n",
            "Iteration 74, loss = 0.73577167\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.22964846\n",
            "Iteration 2, loss = 0.88834888\n",
            "Iteration 3, loss = 0.82907971\n",
            "Iteration 4, loss = 0.80449370\n",
            "Iteration 5, loss = 0.79099376\n",
            "Iteration 6, loss = 0.78327105\n",
            "Iteration 7, loss = 0.77823993\n",
            "Iteration 8, loss = 0.77396604\n",
            "Iteration 9, loss = 0.77124235\n",
            "Iteration 10, loss = 0.76877551\n",
            "Iteration 11, loss = 0.76577172\n",
            "Iteration 12, loss = 0.76429106\n",
            "Iteration 13, loss = 0.76189930\n",
            "Iteration 14, loss = 0.76074958\n",
            "Iteration 15, loss = 0.75841355\n",
            "Iteration 16, loss = 0.75813454\n",
            "Iteration 17, loss = 0.75755104\n",
            "Iteration 18, loss = 0.75689676\n",
            "Iteration 19, loss = 0.75482622\n",
            "Iteration 20, loss = 0.75339559\n",
            "Iteration 21, loss = 0.75413433\n",
            "Iteration 22, loss = 0.75279095\n",
            "Iteration 23, loss = 0.75360062\n",
            "Iteration 24, loss = 0.75059262\n",
            "Iteration 25, loss = 0.75061792\n",
            "Iteration 26, loss = 0.75001802\n",
            "Iteration 27, loss = 0.74750272\n",
            "Iteration 28, loss = 0.74885924\n",
            "Iteration 29, loss = 0.74839812\n",
            "Iteration 30, loss = 0.74614589\n",
            "Iteration 31, loss = 0.74648124\n",
            "Iteration 32, loss = 0.74621359\n",
            "Iteration 33, loss = 0.74422725\n",
            "Iteration 34, loss = 0.74669011\n",
            "Iteration 35, loss = 0.74398335\n",
            "Iteration 36, loss = 0.74476027\n",
            "Iteration 37, loss = 0.74328005\n",
            "Iteration 38, loss = 0.74381182\n",
            "Iteration 39, loss = 0.74343255\n",
            "Iteration 40, loss = 0.74291811\n",
            "Iteration 41, loss = 0.74316658\n",
            "Iteration 42, loss = 0.74376906\n",
            "Iteration 43, loss = 0.74083007\n",
            "Iteration 44, loss = 0.74113887\n",
            "Iteration 45, loss = 0.74109896\n",
            "Iteration 46, loss = 0.73969282\n",
            "Iteration 47, loss = 0.73963081\n",
            "Iteration 48, loss = 0.73971282\n",
            "Iteration 49, loss = 0.74044666\n",
            "Iteration 50, loss = 0.73925716\n",
            "Iteration 51, loss = 0.73943330\n",
            "Iteration 52, loss = 0.73785893\n",
            "Iteration 53, loss = 0.73907211\n",
            "Iteration 54, loss = 0.74040185\n",
            "Iteration 55, loss = 0.73774744\n",
            "Iteration 56, loss = 0.73971662\n",
            "Iteration 57, loss = 0.73945190\n",
            "Iteration 58, loss = 0.73841910\n",
            "Iteration 59, loss = 0.73881688\n",
            "Iteration 60, loss = 0.73847898\n",
            "Iteration 61, loss = 0.73816647\n",
            "Iteration 62, loss = 0.73879879\n",
            "Iteration 63, loss = 0.73694397\n",
            "Iteration 64, loss = 0.73809086\n",
            "Iteration 65, loss = 0.73650045\n",
            "Iteration 66, loss = 0.73636420\n",
            "Iteration 67, loss = 0.73839784\n",
            "Iteration 68, loss = 0.73644801\n",
            "Iteration 69, loss = 0.73690358\n",
            "Iteration 70, loss = 0.73779938\n",
            "Iteration 71, loss = 0.73720779\n",
            "Iteration 72, loss = 0.73695376\n",
            "Iteration 73, loss = 0.73771060\n",
            "Iteration 74, loss = 0.73774757\n",
            "Iteration 75, loss = 0.73691044\n",
            "Iteration 76, loss = 0.73497230\n",
            "Iteration 77, loss = 0.73722513\n",
            "Iteration 78, loss = 0.73625293\n",
            "Iteration 79, loss = 0.73671431\n",
            "Iteration 80, loss = 0.73534459\n",
            "Iteration 81, loss = 0.73647416\n",
            "Iteration 82, loss = 0.73563852\n",
            "Iteration 83, loss = 0.73593132\n",
            "Iteration 84, loss = 0.73553054\n",
            "Iteration 85, loss = 0.73636958\n",
            "Iteration 86, loss = 0.73419602\n",
            "Iteration 87, loss = 0.73514773\n",
            "Iteration 88, loss = 0.73559425\n",
            "Iteration 89, loss = 0.73421311\n",
            "Iteration 90, loss = 0.73458377\n",
            "Iteration 91, loss = 0.73480172\n",
            "Iteration 92, loss = 0.73446286\n",
            "Iteration 93, loss = 0.73470832\n",
            "Iteration 94, loss = 0.73302477\n",
            "Iteration 95, loss = 0.73465036\n",
            "Iteration 96, loss = 0.73332616\n",
            "Iteration 97, loss = 0.73282788\n",
            "Iteration 98, loss = 0.73377648\n",
            "Iteration 99, loss = 0.73256033\n",
            "Iteration 100, loss = 0.73428448\n",
            "Iteration 101, loss = 0.73461811\n",
            "Iteration 102, loss = 0.73420185\n",
            "Iteration 103, loss = 0.73141773\n",
            "Iteration 104, loss = 0.73179435\n",
            "Iteration 105, loss = 0.73205170\n",
            "Iteration 106, loss = 0.73319860\n",
            "Iteration 107, loss = 0.73128520\n",
            "Iteration 108, loss = 0.73068324\n",
            "Iteration 109, loss = 0.73261163\n",
            "Iteration 110, loss = 0.73010486\n",
            "Iteration 111, loss = 0.73058377\n",
            "Iteration 112, loss = 0.73051540\n",
            "Iteration 113, loss = 0.73119653\n",
            "Iteration 114, loss = 0.73078510\n",
            "Iteration 115, loss = 0.72982371\n",
            "Iteration 116, loss = 0.72997137\n",
            "Iteration 117, loss = 0.73138134\n",
            "Iteration 118, loss = 0.72835984\n",
            "Iteration 119, loss = 0.73190192\n",
            "Iteration 120, loss = 0.72971079\n",
            "Iteration 121, loss = 0.72904430\n",
            "Iteration 122, loss = 0.72970292\n",
            "Iteration 123, loss = 0.72994705\n",
            "Iteration 124, loss = 0.72841661\n",
            "Iteration 125, loss = 0.72813407\n",
            "Iteration 126, loss = 0.72780763\n",
            "Iteration 127, loss = 0.72797580\n",
            "Iteration 128, loss = 0.72828684\n",
            "Iteration 129, loss = 0.72953379\n",
            "Iteration 130, loss = 0.72767983\n",
            "Iteration 131, loss = 0.72848698\n",
            "Iteration 132, loss = 0.72834463\n",
            "Iteration 133, loss = 0.72606257\n",
            "Iteration 134, loss = 0.73158010\n",
            "Iteration 135, loss = 0.72735343\n",
            "Iteration 136, loss = 0.72703853\n",
            "Iteration 137, loss = 0.72716402\n",
            "Iteration 138, loss = 0.72737340\n",
            "Iteration 139, loss = 0.72719100\n",
            "Iteration 140, loss = 0.72725063\n",
            "Iteration 141, loss = 0.72859916\n",
            "Iteration 142, loss = 0.72803500\n",
            "Iteration 143, loss = 0.72572076\n",
            "Iteration 144, loss = 0.72723552\n",
            "Iteration 145, loss = 0.72785536\n",
            "Iteration 146, loss = 0.72739450\n",
            "Iteration 147, loss = 0.72708988\n",
            "Iteration 148, loss = 0.72699213\n",
            "Iteration 149, loss = 0.72627741\n",
            "Iteration 150, loss = 0.72796402\n",
            "Iteration 151, loss = 0.72742041\n",
            "Iteration 152, loss = 0.72832916\n",
            "Iteration 153, loss = 0.72848063\n",
            "Iteration 154, loss = 0.72774283\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.23121382\n",
            "Iteration 2, loss = 0.88992832\n",
            "Iteration 3, loss = 0.82965314\n",
            "Iteration 4, loss = 0.80471901\n",
            "Iteration 5, loss = 0.79164616\n",
            "Iteration 6, loss = 0.78393887\n",
            "Iteration 7, loss = 0.77927749\n",
            "Iteration 8, loss = 0.77439798\n",
            "Iteration 9, loss = 0.77210651\n",
            "Iteration 10, loss = 0.76894353\n",
            "Iteration 11, loss = 0.76669534\n",
            "Iteration 12, loss = 0.76534343\n",
            "Iteration 13, loss = 0.76365431\n",
            "Iteration 14, loss = 0.76078526\n",
            "Iteration 15, loss = 0.75997565\n",
            "Iteration 16, loss = 0.75946538\n",
            "Iteration 17, loss = 0.75935453\n",
            "Iteration 18, loss = 0.75791859\n",
            "Iteration 19, loss = 0.75615143\n",
            "Iteration 20, loss = 0.75553574\n",
            "Iteration 21, loss = 0.75447967\n",
            "Iteration 22, loss = 0.75351036\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 23, loss = 0.75458434\n",
            "Iteration 24, loss = 0.75173615\n",
            "Iteration 25, loss = 0.75100863\n",
            "Iteration 26, loss = 0.75090224\n",
            "Iteration 27, loss = 0.74937165\n",
            "Iteration 28, loss = 0.74863059\n",
            "Iteration 29, loss = 0.74872582\n",
            "Iteration 30, loss = 0.74677837\n",
            "Iteration 31, loss = 0.74655690\n",
            "Iteration 32, loss = 0.74619047\n",
            "Iteration 33, loss = 0.74472229\n",
            "Iteration 34, loss = 0.74676111\n",
            "Iteration 35, loss = 0.74348048\n",
            "Iteration 36, loss = 0.74631285\n",
            "Iteration 37, loss = 0.74310926\n",
            "Iteration 38, loss = 0.74319701\n",
            "Iteration 39, loss = 0.74322022\n",
            "Iteration 40, loss = 0.74314685\n",
            "Iteration 41, loss = 0.74309648\n",
            "Iteration 42, loss = 0.74353465\n",
            "Iteration 43, loss = 0.74089463\n",
            "Iteration 44, loss = 0.74052850\n",
            "Iteration 45, loss = 0.74155938\n",
            "Iteration 46, loss = 0.74078301\n",
            "Iteration 47, loss = 0.73997698\n",
            "Iteration 48, loss = 0.73954862\n",
            "Iteration 49, loss = 0.74043967\n",
            "Iteration 50, loss = 0.73878855\n",
            "Iteration 51, loss = 0.73939634\n",
            "Iteration 52, loss = 0.73791715\n",
            "Iteration 53, loss = 0.73909839\n",
            "Iteration 54, loss = 0.74028776\n",
            "Iteration 55, loss = 0.73759181\n",
            "Iteration 56, loss = 0.73901305\n",
            "Iteration 57, loss = 0.73901897\n",
            "Iteration 58, loss = 0.73845906\n",
            "Iteration 59, loss = 0.73830213\n",
            "Iteration 60, loss = 0.73823033\n",
            "Iteration 61, loss = 0.73755573\n",
            "Iteration 62, loss = 0.73890860\n",
            "Iteration 63, loss = 0.73755935\n",
            "Iteration 64, loss = 0.73729946\n",
            "Iteration 65, loss = 0.73624316\n",
            "Iteration 66, loss = 0.73668335\n",
            "Iteration 67, loss = 0.73816507\n",
            "Iteration 68, loss = 0.73591398\n",
            "Iteration 69, loss = 0.73678102\n",
            "Iteration 70, loss = 0.73690560\n",
            "Iteration 71, loss = 0.73588388\n",
            "Iteration 72, loss = 0.73636775\n",
            "Iteration 73, loss = 0.73768843\n",
            "Iteration 74, loss = 0.73781658\n",
            "Iteration 75, loss = 0.73595220\n",
            "Iteration 76, loss = 0.73394019\n",
            "Iteration 77, loss = 0.73634228\n",
            "Iteration 78, loss = 0.73503233\n",
            "Iteration 79, loss = 0.73532402\n",
            "Iteration 80, loss = 0.73395703\n",
            "Iteration 81, loss = 0.73477601\n",
            "Iteration 82, loss = 0.73355858\n",
            "Iteration 83, loss = 0.73495689\n",
            "Iteration 84, loss = 0.73469765\n",
            "Iteration 85, loss = 0.73383107\n",
            "Iteration 86, loss = 0.73324120\n",
            "Iteration 87, loss = 0.73373483\n",
            "Iteration 88, loss = 0.73322339\n",
            "Iteration 89, loss = 0.73377169\n",
            "Iteration 90, loss = 0.73377896\n",
            "Iteration 91, loss = 0.73370753\n",
            "Iteration 92, loss = 0.73230368\n",
            "Iteration 93, loss = 0.73300137\n",
            "Iteration 94, loss = 0.73141111\n",
            "Iteration 95, loss = 0.73318975\n",
            "Iteration 96, loss = 0.73044161\n",
            "Iteration 97, loss = 0.73046307\n",
            "Iteration 98, loss = 0.73214455\n",
            "Iteration 99, loss = 0.73119403\n",
            "Iteration 100, loss = 0.73124945\n",
            "Iteration 101, loss = 0.73249587\n",
            "Iteration 102, loss = 0.73116401\n",
            "Iteration 103, loss = 0.73010689\n",
            "Iteration 104, loss = 0.72994961\n",
            "Iteration 105, loss = 0.73027934\n",
            "Iteration 106, loss = 0.73053147\n",
            "Iteration 107, loss = 0.72911235\n",
            "Iteration 108, loss = 0.72960412\n",
            "Iteration 109, loss = 0.73078693\n",
            "Iteration 110, loss = 0.72831646\n",
            "Iteration 111, loss = 0.72865883\n",
            "Iteration 112, loss = 0.72982924\n",
            "Iteration 113, loss = 0.72986839\n",
            "Iteration 114, loss = 0.72983675\n",
            "Iteration 115, loss = 0.72986756\n",
            "Iteration 116, loss = 0.72964251\n",
            "Iteration 117, loss = 0.73036468\n",
            "Iteration 118, loss = 0.72779488\n",
            "Iteration 119, loss = 0.73039279\n",
            "Iteration 120, loss = 0.72960971\n",
            "Iteration 121, loss = 0.72915425\n",
            "Iteration 122, loss = 0.72955250\n",
            "Iteration 123, loss = 0.72856498\n",
            "Iteration 124, loss = 0.72790781\n",
            "Iteration 125, loss = 0.72773049\n",
            "Iteration 126, loss = 0.72799918\n",
            "Iteration 127, loss = 0.72716698\n",
            "Iteration 128, loss = 0.72842416\n",
            "Iteration 129, loss = 0.72932982\n",
            "Iteration 130, loss = 0.72708515\n",
            "Iteration 131, loss = 0.72879069\n",
            "Iteration 132, loss = 0.72804690\n",
            "Iteration 133, loss = 0.72639063\n",
            "Iteration 134, loss = 0.72986600\n",
            "Iteration 135, loss = 0.72762765\n",
            "Iteration 136, loss = 0.72742986\n",
            "Iteration 137, loss = 0.72754041\n",
            "Iteration 138, loss = 0.72778205\n",
            "Iteration 139, loss = 0.72782740\n",
            "Iteration 140, loss = 0.72777798\n",
            "Iteration 141, loss = 0.72883862\n",
            "Iteration 142, loss = 0.72774103\n",
            "Iteration 143, loss = 0.72658815\n",
            "Iteration 144, loss = 0.72691362\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.24730557\n",
            "Iteration 2, loss = 0.94726646\n",
            "Iteration 3, loss = 0.86821088\n",
            "Iteration 4, loss = 0.82656085\n",
            "Iteration 5, loss = 0.80375382\n",
            "Iteration 6, loss = 0.78907804\n",
            "Iteration 7, loss = 0.78207764\n",
            "Iteration 8, loss = 0.77639382\n",
            "Iteration 9, loss = 0.77230768\n",
            "Iteration 10, loss = 0.76815172\n",
            "Iteration 11, loss = 0.76512457\n",
            "Iteration 12, loss = 0.76359810\n",
            "Iteration 13, loss = 0.76282732\n",
            "Iteration 14, loss = 0.75929619\n",
            "Iteration 15, loss = 0.75890421\n",
            "Iteration 16, loss = 0.75720110\n",
            "Iteration 17, loss = 0.75686519\n",
            "Iteration 18, loss = 0.75635238\n",
            "Iteration 19, loss = 0.75428320\n",
            "Iteration 20, loss = 0.75353804\n",
            "Iteration 21, loss = 0.75259928\n",
            "Iteration 22, loss = 0.75326203\n",
            "Iteration 23, loss = 0.75247902\n",
            "Iteration 24, loss = 0.75084193\n",
            "Iteration 25, loss = 0.75075229\n",
            "Iteration 26, loss = 0.74998432\n",
            "Iteration 27, loss = 0.74923668\n",
            "Iteration 28, loss = 0.74863264\n",
            "Iteration 29, loss = 0.74827787\n",
            "Iteration 30, loss = 0.74737992\n",
            "Iteration 31, loss = 0.74729534\n",
            "Iteration 32, loss = 0.74690318\n",
            "Iteration 33, loss = 0.74545943\n",
            "Iteration 34, loss = 0.74703773\n",
            "Iteration 35, loss = 0.74488621\n",
            "Iteration 36, loss = 0.74697630\n",
            "Iteration 37, loss = 0.74431016\n",
            "Iteration 38, loss = 0.74499764\n",
            "Iteration 39, loss = 0.74453707\n",
            "Iteration 40, loss = 0.74422879\n",
            "Iteration 41, loss = 0.74430361\n",
            "Iteration 42, loss = 0.74484138\n",
            "Iteration 43, loss = 0.74203432\n",
            "Iteration 44, loss = 0.74277128\n",
            "Iteration 45, loss = 0.74265233\n",
            "Iteration 46, loss = 0.74179590\n",
            "Iteration 47, loss = 0.74107874\n",
            "Iteration 48, loss = 0.74248336\n",
            "Iteration 49, loss = 0.74086124\n",
            "Iteration 50, loss = 0.74020669\n",
            "Iteration 51, loss = 0.74068927\n",
            "Iteration 52, loss = 0.73946900\n",
            "Iteration 53, loss = 0.73935148\n",
            "Iteration 54, loss = 0.74129323\n",
            "Iteration 55, loss = 0.73847477\n",
            "Iteration 56, loss = 0.74038060\n",
            "Iteration 57, loss = 0.73953023\n",
            "Iteration 58, loss = 0.74006765\n",
            "Iteration 59, loss = 0.74019448\n",
            "Iteration 60, loss = 0.73862274\n",
            "Iteration 61, loss = 0.73810116\n",
            "Iteration 62, loss = 0.73923073\n",
            "Iteration 63, loss = 0.73908536\n",
            "Iteration 64, loss = 0.73837983\n",
            "Iteration 65, loss = 0.73879381\n",
            "Iteration 66, loss = 0.73745004\n",
            "Iteration 67, loss = 0.73932124\n",
            "Iteration 68, loss = 0.73720059\n",
            "Iteration 69, loss = 0.73693365\n",
            "Iteration 70, loss = 0.73917726\n",
            "Iteration 71, loss = 0.73996253\n",
            "Iteration 72, loss = 0.73695239\n",
            "Iteration 73, loss = 0.73929920\n",
            "Iteration 74, loss = 0.74019913\n",
            "Iteration 75, loss = 0.73692284\n",
            "Iteration 76, loss = 0.73566689\n",
            "Iteration 77, loss = 0.73833349\n",
            "Iteration 78, loss = 0.73683287\n",
            "Iteration 79, loss = 0.73779580\n",
            "Iteration 80, loss = 0.73573982\n",
            "Iteration 81, loss = 0.73694928\n",
            "Iteration 82, loss = 0.73588484\n",
            "Iteration 83, loss = 0.73656856\n",
            "Iteration 84, loss = 0.73627165\n",
            "Iteration 85, loss = 0.73590849\n",
            "Iteration 86, loss = 0.73425517\n",
            "Iteration 87, loss = 0.73538981\n",
            "Iteration 88, loss = 0.73480959\n",
            "Iteration 89, loss = 0.73581396\n",
            "Iteration 90, loss = 0.73568883\n",
            "Iteration 91, loss = 0.73460854\n",
            "Iteration 92, loss = 0.73444853\n",
            "Iteration 93, loss = 0.73482350\n",
            "Iteration 94, loss = 0.73395481\n",
            "Iteration 95, loss = 0.73384289\n",
            "Iteration 96, loss = 0.73277303\n",
            "Iteration 97, loss = 0.73289866\n",
            "Iteration 98, loss = 0.73324813\n",
            "Iteration 99, loss = 0.73286074\n",
            "Iteration 100, loss = 0.73308416\n",
            "Iteration 101, loss = 0.73187899\n",
            "Iteration 102, loss = 0.73268316\n",
            "Iteration 103, loss = 0.73186063\n",
            "Iteration 104, loss = 0.73187802\n",
            "Iteration 105, loss = 0.73103868\n",
            "Iteration 106, loss = 0.73156034\n",
            "Iteration 107, loss = 0.72972118\n",
            "Iteration 108, loss = 0.73090930\n",
            "Iteration 109, loss = 0.73193071\n",
            "Iteration 110, loss = 0.72942225\n",
            "Iteration 111, loss = 0.72872511\n",
            "Iteration 112, loss = 0.72995096\n",
            "Iteration 113, loss = 0.73054370\n",
            "Iteration 114, loss = 0.73107444\n",
            "Iteration 115, loss = 0.72950236\n",
            "Iteration 116, loss = 0.72932389\n",
            "Iteration 117, loss = 0.73046428\n",
            "Iteration 118, loss = 0.72773355\n",
            "Iteration 119, loss = 0.73127806\n",
            "Iteration 120, loss = 0.73037963\n",
            "Iteration 121, loss = 0.72906666\n",
            "Iteration 122, loss = 0.73009042\n",
            "Iteration 123, loss = 0.72918699\n",
            "Iteration 124, loss = 0.72841681\n",
            "Iteration 125, loss = 0.72806050\n",
            "Iteration 126, loss = 0.72854764\n",
            "Iteration 127, loss = 0.72740292\n",
            "Iteration 128, loss = 0.72845474\n",
            "Iteration 129, loss = 0.72857935\n",
            "Iteration 130, loss = 0.72742813\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 131, loss = 0.72836190\n",
            "Iteration 132, loss = 0.72927415\n",
            "Iteration 133, loss = 0.72723211\n",
            "Iteration 134, loss = 0.72921624\n",
            "Iteration 135, loss = 0.72666557\n",
            "Iteration 136, loss = 0.72701655\n",
            "Iteration 137, loss = 0.72816838\n",
            "Iteration 138, loss = 0.72757803\n",
            "Iteration 139, loss = 0.72672947\n",
            "Iteration 140, loss = 0.72798810\n",
            "Iteration 141, loss = 0.72892921\n",
            "Iteration 142, loss = 0.72806534\n",
            "Iteration 143, loss = 0.72721037\n",
            "Iteration 144, loss = 0.72762632\n",
            "Iteration 145, loss = 0.72865262\n",
            "Iteration 146, loss = 0.72668323\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.25290308\n",
            "Iteration 2, loss = 0.98591020\n",
            "Iteration 3, loss = 0.90812424\n",
            "Iteration 4, loss = 0.87555046\n",
            "Iteration 5, loss = 0.86319763\n",
            "Iteration 6, loss = 0.85545830\n",
            "Iteration 7, loss = 0.85176978\n",
            "Iteration 8, loss = 0.84827293\n",
            "Iteration 9, loss = 0.84746007\n",
            "Iteration 10, loss = 0.84487249\n",
            "Iteration 11, loss = 0.84244295\n",
            "Iteration 12, loss = 0.84085678\n",
            "Iteration 13, loss = 0.84151976\n",
            "Iteration 14, loss = 0.83955740\n",
            "Iteration 15, loss = 0.83786095\n",
            "Iteration 16, loss = 0.83606952\n",
            "Iteration 17, loss = 0.83348526\n",
            "Iteration 18, loss = 0.82911799\n",
            "Iteration 19, loss = 0.82351157\n",
            "Iteration 20, loss = 0.82049024\n",
            "Iteration 21, loss = 0.81802714\n",
            "Iteration 22, loss = 0.81658183\n",
            "Iteration 23, loss = 0.81489317\n",
            "Iteration 24, loss = 0.81458161\n",
            "Iteration 25, loss = 0.81146387\n",
            "Iteration 26, loss = 0.81002021\n",
            "Iteration 27, loss = 0.80714682\n",
            "Iteration 28, loss = 0.80515817\n",
            "Iteration 29, loss = 0.80408267\n",
            "Iteration 30, loss = 0.80398623\n",
            "Iteration 31, loss = 0.80229119\n",
            "Iteration 32, loss = 0.80037010\n",
            "Iteration 33, loss = 0.79989031\n",
            "Iteration 34, loss = 0.79931198\n",
            "Iteration 35, loss = 0.79686874\n",
            "Iteration 36, loss = 0.79843415\n",
            "Iteration 37, loss = 0.79675161\n",
            "Iteration 38, loss = 0.79643823\n",
            "Iteration 39, loss = 0.79612361\n",
            "Iteration 40, loss = 0.79562832\n",
            "Iteration 41, loss = 0.79505424\n",
            "Iteration 42, loss = 0.79545512\n",
            "Iteration 43, loss = 0.79395559\n",
            "Iteration 44, loss = 0.79500819\n",
            "Iteration 45, loss = 0.79361481\n",
            "Iteration 46, loss = 0.79317492\n",
            "Iteration 47, loss = 0.79232359\n",
            "Iteration 48, loss = 0.79356670\n",
            "Iteration 49, loss = 0.79401472\n",
            "Iteration 50, loss = 0.79337161\n",
            "Iteration 51, loss = 0.79089289\n",
            "Iteration 52, loss = 0.79318075\n",
            "Iteration 53, loss = 0.79128182\n",
            "Iteration 54, loss = 0.79350812\n",
            "Iteration 55, loss = 0.79129755\n",
            "Iteration 56, loss = 0.79111380\n",
            "Iteration 57, loss = 0.79059548\n",
            "Iteration 58, loss = 0.79117205\n",
            "Iteration 59, loss = 0.79162308\n",
            "Iteration 60, loss = 0.79047379\n",
            "Iteration 61, loss = 0.79133971\n",
            "Iteration 62, loss = 0.79109734\n",
            "Iteration 63, loss = 0.79079718\n",
            "Iteration 64, loss = 0.78957346\n",
            "Iteration 65, loss = 0.78988395\n",
            "Iteration 66, loss = 0.78873786\n",
            "Iteration 67, loss = 0.78985354\n",
            "Iteration 68, loss = 0.78957284\n",
            "Iteration 69, loss = 0.78969740\n",
            "Iteration 70, loss = 0.78906687\n",
            "Iteration 71, loss = 0.78880733\n",
            "Iteration 72, loss = 0.78853635\n",
            "Iteration 73, loss = 0.78868977\n",
            "Iteration 74, loss = 0.78925633\n",
            "Iteration 75, loss = 0.78830569\n",
            "Iteration 76, loss = 0.78888566\n",
            "Iteration 77, loss = 0.78909198\n",
            "Iteration 78, loss = 0.78891634\n",
            "Iteration 79, loss = 0.78900251\n",
            "Iteration 80, loss = 0.78670599\n",
            "Iteration 81, loss = 0.78901352\n",
            "Iteration 82, loss = 0.78611085\n",
            "Iteration 83, loss = 0.78881006\n",
            "Iteration 84, loss = 0.78827269\n",
            "Iteration 85, loss = 0.78839704\n",
            "Iteration 86, loss = 0.78734282\n",
            "Iteration 87, loss = 0.78684813\n",
            "Iteration 88, loss = 0.78642487\n",
            "Iteration 89, loss = 0.78749868\n",
            "Iteration 90, loss = 0.78851525\n",
            "Iteration 91, loss = 0.78650478\n",
            "Iteration 92, loss = 0.78706779\n",
            "Iteration 93, loss = 0.78649923\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.25504199\n",
            "Iteration 2, loss = 0.98515356\n",
            "Iteration 3, loss = 0.92596911\n",
            "Iteration 4, loss = 0.87507864\n",
            "Iteration 5, loss = 0.84942131\n",
            "Iteration 6, loss = 0.82713622\n",
            "Iteration 7, loss = 0.80926860\n",
            "Iteration 8, loss = 0.79482422\n",
            "Iteration 9, loss = 0.78464521\n",
            "Iteration 10, loss = 0.77730196\n",
            "Iteration 11, loss = 0.76891536\n",
            "Iteration 12, loss = 0.76224057\n",
            "Iteration 13, loss = 0.75547341\n",
            "Iteration 14, loss = 0.75228377\n",
            "Iteration 15, loss = 0.74782564\n",
            "Iteration 16, loss = 0.74470373\n",
            "Iteration 17, loss = 0.74075459\n",
            "Iteration 18, loss = 0.73940825\n",
            "Iteration 19, loss = 0.73825466\n",
            "Iteration 20, loss = 0.73633863\n",
            "Iteration 21, loss = 0.73496950\n",
            "Iteration 22, loss = 0.73464843\n",
            "Iteration 23, loss = 0.73336772\n",
            "Iteration 24, loss = 0.73226910\n",
            "Iteration 25, loss = 0.73214831\n",
            "Iteration 26, loss = 0.73023800\n",
            "Iteration 27, loss = 0.72903441\n",
            "Iteration 28, loss = 0.72781291\n",
            "Iteration 29, loss = 0.72934879\n",
            "Iteration 30, loss = 0.72700369\n",
            "Iteration 31, loss = 0.72877867\n",
            "Iteration 32, loss = 0.72764193\n",
            "Iteration 33, loss = 0.72708311\n",
            "Iteration 34, loss = 0.72762631\n",
            "Iteration 35, loss = 0.72580934\n",
            "Iteration 36, loss = 0.72682429\n",
            "Iteration 37, loss = 0.72523690\n",
            "Iteration 38, loss = 0.72757724\n",
            "Iteration 39, loss = 0.72684740\n",
            "Iteration 40, loss = 0.72633029\n",
            "Iteration 41, loss = 0.72635770\n",
            "Iteration 42, loss = 0.72498740\n",
            "Iteration 43, loss = 0.72597120\n",
            "Iteration 44, loss = 0.72563169\n",
            "Iteration 45, loss = 0.72503292\n",
            "Iteration 46, loss = 0.72486700\n",
            "Iteration 47, loss = 0.72512146\n",
            "Iteration 48, loss = 0.72464769\n",
            "Iteration 49, loss = 0.72506695\n",
            "Iteration 50, loss = 0.72311099\n",
            "Iteration 51, loss = 0.72453952\n",
            "Iteration 52, loss = 0.72433288\n",
            "Iteration 53, loss = 0.72562611\n",
            "Iteration 54, loss = 0.72532110\n",
            "Iteration 55, loss = 0.72377599\n",
            "Iteration 56, loss = 0.72454449\n",
            "Iteration 57, loss = 0.72390281\n",
            "Iteration 58, loss = 0.72466208\n",
            "Iteration 59, loss = 0.72466461\n",
            "Iteration 60, loss = 0.72403202\n",
            "Iteration 61, loss = 0.72495780\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.19200554\n",
            "Iteration 2, loss = 0.84289670\n",
            "Iteration 3, loss = 0.79983937\n",
            "Iteration 4, loss = 0.78360293\n",
            "Iteration 5, loss = 0.77489818\n",
            "Iteration 6, loss = 0.77026288\n",
            "Iteration 7, loss = 0.76654322\n",
            "Iteration 8, loss = 0.76346380\n",
            "Iteration 9, loss = 0.76129554\n",
            "Iteration 10, loss = 0.75889421\n",
            "Iteration 11, loss = 0.75771188\n",
            "Iteration 12, loss = 0.75512135\n",
            "Iteration 13, loss = 0.75427849\n",
            "Iteration 14, loss = 0.75378052\n",
            "Iteration 15, loss = 0.75284271\n",
            "Iteration 16, loss = 0.75346482\n",
            "Iteration 17, loss = 0.75011248\n",
            "Iteration 18, loss = 0.75069344\n",
            "Iteration 19, loss = 0.75059083\n",
            "Iteration 20, loss = 0.74852803\n",
            "Iteration 21, loss = 0.74824865\n",
            "Iteration 22, loss = 0.74853037\n",
            "Iteration 23, loss = 0.74698626\n",
            "Iteration 24, loss = 0.74501713\n",
            "Iteration 25, loss = 0.74615719\n",
            "Iteration 26, loss = 0.74464353\n",
            "Iteration 27, loss = 0.74373538\n",
            "Iteration 28, loss = 0.74066459\n",
            "Iteration 29, loss = 0.74143071\n",
            "Iteration 30, loss = 0.74034991\n",
            "Iteration 31, loss = 0.74108773\n",
            "Iteration 32, loss = 0.73964236\n",
            "Iteration 33, loss = 0.73799520\n",
            "Iteration 34, loss = 0.73970536\n",
            "Iteration 35, loss = 0.73790584\n",
            "Iteration 36, loss = 0.73732238\n",
            "Iteration 37, loss = 0.73665422\n",
            "Iteration 38, loss = 0.73690001\n",
            "Iteration 39, loss = 0.73763882\n",
            "Iteration 40, loss = 0.73614363\n",
            "Iteration 41, loss = 0.73561432\n",
            "Iteration 42, loss = 0.73455759\n",
            "Iteration 43, loss = 0.73455484\n",
            "Iteration 44, loss = 0.73391257\n",
            "Iteration 45, loss = 0.73475497\n",
            "Iteration 46, loss = 0.73188114\n",
            "Iteration 47, loss = 0.73212829\n",
            "Iteration 48, loss = 0.73151120\n",
            "Iteration 49, loss = 0.73047159\n",
            "Iteration 50, loss = 0.72945523\n",
            "Iteration 51, loss = 0.73017436\n",
            "Iteration 52, loss = 0.73025616\n",
            "Iteration 53, loss = 0.73052665\n",
            "Iteration 54, loss = 0.73103328\n",
            "Iteration 55, loss = 0.72939996\n",
            "Iteration 56, loss = 0.72868987\n",
            "Iteration 57, loss = 0.72879197\n",
            "Iteration 58, loss = 0.72830247\n",
            "Iteration 59, loss = 0.72977316\n",
            "Iteration 60, loss = 0.72743939\n",
            "Iteration 61, loss = 0.72757013\n",
            "Iteration 62, loss = 0.72819204\n",
            "Iteration 63, loss = 0.72836102\n",
            "Iteration 64, loss = 0.72603280\n",
            "Iteration 65, loss = 0.72724416\n",
            "Iteration 66, loss = 0.72659299\n",
            "Iteration 67, loss = 0.72762499\n",
            "Iteration 68, loss = 0.72653987\n",
            "Iteration 69, loss = 0.72581475\n",
            "Iteration 70, loss = 0.72710333\n",
            "Iteration 71, loss = 0.72746533\n",
            "Iteration 72, loss = 0.72588586\n",
            "Iteration 73, loss = 0.72743799\n",
            "Iteration 74, loss = 0.72778785\n",
            "Iteration 75, loss = 0.72507831\n",
            "Iteration 76, loss = 0.72464668\n",
            "Iteration 77, loss = 0.72702146\n",
            "Iteration 78, loss = 0.72513523\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 79, loss = 0.72719101\n",
            "Iteration 80, loss = 0.72483968\n",
            "Iteration 81, loss = 0.72453620\n",
            "Iteration 82, loss = 0.72378371\n",
            "Iteration 83, loss = 0.72509500\n",
            "Iteration 84, loss = 0.72704528\n",
            "Iteration 85, loss = 0.72542808\n",
            "Iteration 86, loss = 0.72536536\n",
            "Iteration 87, loss = 0.72597237\n",
            "Iteration 88, loss = 0.72476827\n",
            "Iteration 89, loss = 0.72511165\n",
            "Iteration 90, loss = 0.72583345\n",
            "Iteration 91, loss = 0.72553823\n",
            "Iteration 92, loss = 0.72483140\n",
            "Iteration 93, loss = 0.72400398\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.18892436\n",
            "Iteration 2, loss = 0.84388525\n",
            "Iteration 3, loss = 0.80060450\n",
            "Iteration 4, loss = 0.78460058\n",
            "Iteration 5, loss = 0.77648444\n",
            "Iteration 6, loss = 0.77100181\n",
            "Iteration 7, loss = 0.76672459\n",
            "Iteration 8, loss = 0.76408478\n",
            "Iteration 9, loss = 0.76155056\n",
            "Iteration 10, loss = 0.75911400\n",
            "Iteration 11, loss = 0.75739642\n",
            "Iteration 12, loss = 0.75634232\n",
            "Iteration 13, loss = 0.75435163\n",
            "Iteration 14, loss = 0.75476765\n",
            "Iteration 15, loss = 0.75390916\n",
            "Iteration 16, loss = 0.75258183\n",
            "Iteration 17, loss = 0.75187159\n",
            "Iteration 18, loss = 0.75088865\n",
            "Iteration 19, loss = 0.75174967\n",
            "Iteration 20, loss = 0.74847411\n",
            "Iteration 21, loss = 0.74917484\n",
            "Iteration 22, loss = 0.74921088\n",
            "Iteration 23, loss = 0.74709200\n",
            "Iteration 24, loss = 0.74630261\n",
            "Iteration 25, loss = 0.74644157\n",
            "Iteration 26, loss = 0.74516376\n",
            "Iteration 27, loss = 0.74459704\n",
            "Iteration 28, loss = 0.74217470\n",
            "Iteration 29, loss = 0.74278122\n",
            "Iteration 30, loss = 0.74117280\n",
            "Iteration 31, loss = 0.74153633\n",
            "Iteration 32, loss = 0.74079451\n",
            "Iteration 33, loss = 0.73929746\n",
            "Iteration 34, loss = 0.73999847\n",
            "Iteration 35, loss = 0.73851522\n",
            "Iteration 36, loss = 0.73915410\n",
            "Iteration 37, loss = 0.73802397\n",
            "Iteration 38, loss = 0.73872129\n",
            "Iteration 39, loss = 0.73905719\n",
            "Iteration 40, loss = 0.73766257\n",
            "Iteration 41, loss = 0.73790715\n",
            "Iteration 42, loss = 0.73676976\n",
            "Iteration 43, loss = 0.73529176\n",
            "Iteration 44, loss = 0.73607415\n",
            "Iteration 45, loss = 0.73666441\n",
            "Iteration 46, loss = 0.73361553\n",
            "Iteration 47, loss = 0.73490964\n",
            "Iteration 48, loss = 0.73352068\n",
            "Iteration 49, loss = 0.73203931\n",
            "Iteration 50, loss = 0.73268375\n",
            "Iteration 51, loss = 0.73273472\n",
            "Iteration 52, loss = 0.73195018\n",
            "Iteration 53, loss = 0.73171005\n",
            "Iteration 54, loss = 0.73406836\n",
            "Iteration 55, loss = 0.73159327\n",
            "Iteration 56, loss = 0.73142925\n",
            "Iteration 57, loss = 0.73124259\n",
            "Iteration 58, loss = 0.73025576\n",
            "Iteration 59, loss = 0.73095943\n",
            "Iteration 60, loss = 0.72969567\n",
            "Iteration 61, loss = 0.72980409\n",
            "Iteration 62, loss = 0.73053552\n",
            "Iteration 63, loss = 0.73070702\n",
            "Iteration 64, loss = 0.72807908\n",
            "Iteration 65, loss = 0.72984657\n",
            "Iteration 66, loss = 0.72849184\n",
            "Iteration 67, loss = 0.73014912\n",
            "Iteration 68, loss = 0.72902076\n",
            "Iteration 69, loss = 0.72774825\n",
            "Iteration 70, loss = 0.72987570\n",
            "Iteration 71, loss = 0.72891266\n",
            "Iteration 72, loss = 0.72763525\n",
            "Iteration 73, loss = 0.72862128\n",
            "Iteration 74, loss = 0.72934423\n",
            "Iteration 75, loss = 0.72635637\n",
            "Iteration 76, loss = 0.72683602\n",
            "Iteration 77, loss = 0.72776754\n",
            "Iteration 78, loss = 0.72715436\n",
            "Iteration 79, loss = 0.72815732\n",
            "Iteration 80, loss = 0.72614253\n",
            "Iteration 81, loss = 0.72701996\n",
            "Iteration 82, loss = 0.72615763\n",
            "Iteration 83, loss = 0.72710332\n",
            "Iteration 84, loss = 0.72797656\n",
            "Iteration 85, loss = 0.72639318\n",
            "Iteration 86, loss = 0.72802008\n",
            "Iteration 87, loss = 0.72710537\n",
            "Iteration 88, loss = 0.72672229\n",
            "Iteration 89, loss = 0.72681804\n",
            "Iteration 90, loss = 0.72773412\n",
            "Iteration 91, loss = 0.72719828\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.20662451\n",
            "Iteration 2, loss = 0.84822357\n",
            "Iteration 3, loss = 0.80023095\n",
            "Iteration 4, loss = 0.78375886\n",
            "Iteration 5, loss = 0.77529814\n",
            "Iteration 6, loss = 0.77037675\n",
            "Iteration 7, loss = 0.76508767\n",
            "Iteration 8, loss = 0.76343640\n",
            "Iteration 9, loss = 0.76091858\n",
            "Iteration 10, loss = 0.75794176\n",
            "Iteration 11, loss = 0.75641703\n",
            "Iteration 12, loss = 0.75427780\n",
            "Iteration 13, loss = 0.75384854\n",
            "Iteration 14, loss = 0.75380394\n",
            "Iteration 15, loss = 0.75200794\n",
            "Iteration 16, loss = 0.75174734\n",
            "Iteration 17, loss = 0.75106606\n",
            "Iteration 18, loss = 0.74937166\n",
            "Iteration 19, loss = 0.75033142\n",
            "Iteration 20, loss = 0.74826062\n",
            "Iteration 21, loss = 0.74740562\n",
            "Iteration 22, loss = 0.74702492\n",
            "Iteration 23, loss = 0.74709234\n",
            "Iteration 24, loss = 0.74584320\n",
            "Iteration 25, loss = 0.74517360\n",
            "Iteration 26, loss = 0.74449004\n",
            "Iteration 27, loss = 0.74393567\n",
            "Iteration 28, loss = 0.74247389\n",
            "Iteration 29, loss = 0.74217303\n",
            "Iteration 30, loss = 0.74057053\n",
            "Iteration 31, loss = 0.74030598\n",
            "Iteration 32, loss = 0.74043401\n",
            "Iteration 33, loss = 0.73907266\n",
            "Iteration 34, loss = 0.74090293\n",
            "Iteration 35, loss = 0.73784825\n",
            "Iteration 36, loss = 0.73860590\n",
            "Iteration 37, loss = 0.73839404\n",
            "Iteration 38, loss = 0.73761742\n",
            "Iteration 39, loss = 0.73763233\n",
            "Iteration 40, loss = 0.73633256\n",
            "Iteration 41, loss = 0.73654406\n",
            "Iteration 42, loss = 0.73572966\n",
            "Iteration 43, loss = 0.73459938\n",
            "Iteration 44, loss = 0.73430616\n",
            "Iteration 45, loss = 0.73579557\n",
            "Iteration 46, loss = 0.73312406\n",
            "Iteration 47, loss = 0.73328996\n",
            "Iteration 48, loss = 0.73229831\n",
            "Iteration 49, loss = 0.73076031\n",
            "Iteration 50, loss = 0.73113210\n",
            "Iteration 51, loss = 0.73152626\n",
            "Iteration 52, loss = 0.72985820\n",
            "Iteration 53, loss = 0.73024522\n",
            "Iteration 54, loss = 0.73162460\n",
            "Iteration 55, loss = 0.73002964\n",
            "Iteration 56, loss = 0.72877604\n",
            "Iteration 57, loss = 0.72943536\n",
            "Iteration 58, loss = 0.72878660\n",
            "Iteration 59, loss = 0.72780421\n",
            "Iteration 60, loss = 0.72744142\n",
            "Iteration 61, loss = 0.72728748\n",
            "Iteration 62, loss = 0.72830292\n",
            "Iteration 63, loss = 0.72844749\n",
            "Iteration 64, loss = 0.72544670\n",
            "Iteration 65, loss = 0.72757655\n",
            "Iteration 66, loss = 0.72622129\n",
            "Iteration 67, loss = 0.72687009\n",
            "Iteration 68, loss = 0.72689586\n",
            "Iteration 69, loss = 0.72602061\n",
            "Iteration 70, loss = 0.72708137\n",
            "Iteration 71, loss = 0.72555674\n",
            "Iteration 72, loss = 0.72487462\n",
            "Iteration 73, loss = 0.72606853\n",
            "Iteration 74, loss = 0.72611599\n",
            "Iteration 75, loss = 0.72410653\n",
            "Iteration 76, loss = 0.72460929\n",
            "Iteration 77, loss = 0.72523108\n",
            "Iteration 78, loss = 0.72438038\n",
            "Iteration 79, loss = 0.72512566\n",
            "Iteration 80, loss = 0.72445899\n",
            "Iteration 81, loss = 0.72435685\n",
            "Iteration 82, loss = 0.72345349\n",
            "Iteration 83, loss = 0.72353511\n",
            "Iteration 84, loss = 0.72631382\n",
            "Iteration 85, loss = 0.72289014\n",
            "Iteration 86, loss = 0.72469750\n",
            "Iteration 87, loss = 0.72521777\n",
            "Iteration 88, loss = 0.72434421\n",
            "Iteration 89, loss = 0.72441619\n",
            "Iteration 90, loss = 0.72561547\n",
            "Iteration 91, loss = 0.72487173\n",
            "Iteration 92, loss = 0.72345609\n",
            "Iteration 93, loss = 0.72294446\n",
            "Iteration 94, loss = 0.72428568\n",
            "Iteration 95, loss = 0.72311727\n",
            "Iteration 96, loss = 0.72477565\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "Iteration 1, loss = 1.16940183\n",
            "Iteration 2, loss = 0.84036006\n",
            "Iteration 3, loss = 0.79902902\n",
            "Iteration 4, loss = 0.78403961\n",
            "Iteration 5, loss = 0.77550739\n",
            "Iteration 6, loss = 0.77098243\n",
            "Iteration 7, loss = 0.76604097\n",
            "Iteration 8, loss = 0.76379721\n",
            "Iteration 9, loss = 0.76151167\n",
            "Iteration 10, loss = 0.75831962\n",
            "Iteration 11, loss = 0.75673650\n",
            "Iteration 12, loss = 0.75570282\n",
            "Iteration 13, loss = 0.75358490\n",
            "Iteration 14, loss = 0.75397309\n",
            "Iteration 15, loss = 0.75237146\n",
            "Iteration 16, loss = 0.75319025\n",
            "Iteration 17, loss = 0.75091269\n",
            "Iteration 18, loss = 0.74950470\n",
            "Iteration 19, loss = 0.75011367\n",
            "Iteration 20, loss = 0.74869940\n",
            "Iteration 21, loss = 0.74872677\n",
            "Iteration 22, loss = 0.74840042\n",
            "Iteration 23, loss = 0.74650820\n",
            "Iteration 24, loss = 0.74638300\n",
            "Iteration 25, loss = 0.74651695\n",
            "Iteration 26, loss = 0.74483374\n",
            "Iteration 27, loss = 0.74411509\n",
            "Iteration 28, loss = 0.74313205\n",
            "Iteration 29, loss = 0.74307381\n",
            "Iteration 30, loss = 0.74155104\n",
            "Iteration 31, loss = 0.74142464\n",
            "Iteration 32, loss = 0.74151412\n",
            "Iteration 33, loss = 0.73986835\n",
            "Iteration 34, loss = 0.74157372\n",
            "Iteration 35, loss = 0.73955874\n",
            "Iteration 36, loss = 0.74053837\n",
            "Iteration 37, loss = 0.73937301\n",
            "Iteration 38, loss = 0.73934608\n",
            "Iteration 39, loss = 0.73957287\n",
            "Iteration 40, loss = 0.73639830\n",
            "Iteration 41, loss = 0.73717363\n",
            "Iteration 42, loss = 0.73735607\n",
            "Iteration 43, loss = 0.73614942\n",
            "Iteration 44, loss = 0.73596113\n",
            "Iteration 45, loss = 0.73891310\n",
            "Iteration 46, loss = 0.73524849\n",
            "Iteration 47, loss = 0.73481198\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 48, loss = 0.73500608\n",
            "Iteration 49, loss = 0.73323408\n",
            "Iteration 50, loss = 0.73403443\n",
            "Iteration 51, loss = 0.73346108\n",
            "Iteration 52, loss = 0.73349060\n",
            "Iteration 53, loss = 0.73402092\n",
            "Iteration 54, loss = 0.73346121\n",
            "Iteration 55, loss = 0.73298334\n",
            "Iteration 56, loss = 0.73257965\n",
            "Iteration 57, loss = 0.73220311\n",
            "Iteration 58, loss = 0.73148728\n",
            "Iteration 59, loss = 0.73166969\n",
            "Iteration 60, loss = 0.73144009\n",
            "Iteration 61, loss = 0.72900056\n",
            "Iteration 62, loss = 0.73091197\n",
            "Iteration 63, loss = 0.73115400\n",
            "Iteration 64, loss = 0.72773752\n",
            "Iteration 65, loss = 0.72968785\n",
            "Iteration 66, loss = 0.72970174\n",
            "Iteration 67, loss = 0.72856717\n",
            "Iteration 68, loss = 0.73008992\n",
            "Iteration 69, loss = 0.72859221\n",
            "Iteration 70, loss = 0.72946566\n",
            "Iteration 71, loss = 0.72832753\n",
            "Iteration 72, loss = 0.72715675\n",
            "Iteration 73, loss = 0.72889167\n",
            "Iteration 74, loss = 0.72764593\n",
            "Iteration 75, loss = 0.72697454\n",
            "Iteration 76, loss = 0.72661903\n",
            "Iteration 77, loss = 0.72689491\n",
            "Iteration 78, loss = 0.72662575\n",
            "Iteration 79, loss = 0.72824075\n",
            "Iteration 80, loss = 0.72612335\n",
            "Iteration 81, loss = 0.72683909\n",
            "Iteration 82, loss = 0.72634173\n",
            "Iteration 83, loss = 0.72548601\n",
            "Iteration 84, loss = 0.72684770\n",
            "Iteration 85, loss = 0.72506092\n",
            "Iteration 86, loss = 0.72792448\n",
            "Iteration 87, loss = 0.72649216\n",
            "Iteration 88, loss = 0.72594869\n",
            "Iteration 89, loss = 0.72661646\n",
            "Iteration 90, loss = 0.72707980\n",
            "Iteration 91, loss = 0.72678544\n",
            "Iteration 92, loss = 0.72447291\n",
            "Iteration 93, loss = 0.72629184\n",
            "Iteration 94, loss = 0.72666988\n",
            "Iteration 95, loss = 0.72531541\n",
            "Iteration 96, loss = 0.72684665\n",
            "Iteration 97, loss = 0.72657620\n",
            "Iteration 98, loss = 0.72571995\n",
            "Iteration 99, loss = 0.72589147\n",
            "Iteration 100, loss = 0.72679118\n",
            "Iteration 101, loss = 0.72702800\n",
            "Iteration 102, loss = 0.72631143\n",
            "Iteration 103, loss = 0.72598543\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n",
            "\n",
            "\n",
            " accuracy:  [0.7978  0.80075 0.8018  0.77945 0.76155 0.79135 0.8001  0.79865 0.78995\n",
            " 0.80315]\n"
          ]
        }
      ],
      "source": [
        "# Applying 10-fold cross validation with ANN classifier:\n",
        "\n",
        "my_ANN = MLPClassifier(hidden_layer_sizes=(3,), activation= 'logistic', \n",
        "                       solver='adam', alpha=1e-5, random_state=1, \n",
        "                       learning_rate_init = 0.1, verbose=True, tol=0.0001)\n",
        "\n",
        "# CV:\n",
        "accuracy_list = cross_val_score(my_ANN, X, y, cv=10, scoring='accuracy')\n",
        "\n",
        "print('\\n\\n','accuracy: ',accuracy_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MhlhXXW2rk2_",
        "outputId": "7b45d9ae-a3c7-410c-9b00-0f277586b8a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.792455\n"
          ]
        }
      ],
      "source": [
        "# use average of accuracy values as final result\n",
        "accuracy_cv = accuracy_list.mean()\n",
        "\n",
        "print(accuracy_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsEzOoPB1F8i"
      },
      "source": [
        "## Implement ANN with Keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "74dv6b9r1Jvc",
        "scrolled": false,
        "outputId": "c2da39dc-a04c-4e75-b0d9-bc37c07be9b3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "C:\\Users\\newusername\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ]
        }
      ],
      "source": [
        "from sklearn import preprocessing\n",
        "# import some utilities to transform/preprocess our data:\n",
        "from keras.utils import np_utils\n",
        "\n",
        "# Encoding categorical features\n",
        "le = LabelEncoder()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JmGhFwNu1Ou-"
      },
      "outputs": [],
      "source": [
        "# \"Sequential\" models let us define a stack of neural network layers\n",
        "from keras.models import Sequential\n",
        "\n",
        "# import the core layers:\n",
        "from keras.layers import Dense, Dropout, Activation, Flatten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9jXovZT1P-O",
        "outputId": "41a9c78f-a66e-439e-d838-213c20716aad"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "row_id\n",
              "0           Streptococcus_pyogenes\n",
              "1              Salmonella_enterica\n",
              "2              Salmonella_enterica\n",
              "3              Salmonella_enterica\n",
              "4               Enterococcus_hirae\n",
              "                    ...           \n",
              "199995         Salmonella_enterica\n",
              "199996      Streptococcus_pyogenes\n",
              "199997    Streptococcus_pneumoniae\n",
              "199998       Staphylococcus_aureus\n",
              "199999       Klebsiella_pneumoniae\n",
              "Name: target, Length: 200000, dtype: category\n",
              "Categories (10, object): ['Bacteroides_fragilis', 'Campylobacter_jejuni', 'Enterococcus_hirae', 'Escherichia_coli', ..., 'Salmonella_enterica', 'Staphylococcus_aureus', 'Streptococcus_pneumoniae', 'Streptococcus_pyogenes']"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X = df_train.iloc[:,:-1]\n",
        "y = df_train['target']\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vTc6pPie1TEU",
        "outputId": "6c3f2490-f5b3-40e0-cf4c-1c60fa881a2e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199995</th>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199996</th>\n",
              "      <td>9</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199997</th>\n",
              "      <td>8</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199998</th>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>199999</th>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>200000 rows Ã— 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        target\n",
              "0            9\n",
              "1            6\n",
              "2            6\n",
              "3            6\n",
              "4            2\n",
              "...        ...\n",
              "199995       6\n",
              "199996       9\n",
              "199997       8\n",
              "199998       7\n",
              "199999       5\n",
              "\n",
              "[200000 rows x 1 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y = pd.DataFrame(le.fit_transform(df_train['target']), columns=['target'])\n",
        "y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7-MAuTnH1U-G"
      },
      "outputs": [],
      "source": [
        "# Randomly splitting the original dataset into training set and testing set:\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z-OnyNZ11WrH"
      },
      "outputs": [],
      "source": [
        "# OneHotEncoding for the output label:\n",
        "y_train = np_utils.to_categorical(y_train, 10)\n",
        "y_test = np_utils.to_categorical(y_test, 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pT9Pvpv61X_N"
      },
      "outputs": [],
      "source": [
        "# Declare Sequential model to build our network:\n",
        "model = Sequential()\n",
        "\n",
        "# Compiling\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              optimizer='adam')\n",
        "\n",
        "input_size = 286\n",
        "hidden_neurons = 100\n",
        "out_size = 10\n",
        "\n",
        "## Designing the ANN Structure (with 784 inputs, 10 outputs and 100 neuron in a hidden layer):\n",
        "# -----------------------------------------\n",
        "# first layer: input layer\n",
        "# Input layer does not do any processing, so no need to define the input layer in this problem.\n",
        "\n",
        "# -----------------------------------------\n",
        "# second layer: hidden layer:\n",
        "model.add(Dense(hidden_neurons, input_dim = input_size))  # Nuerons\n",
        "model.add(Activation('sigmoid')) # Activation\n",
        "\n",
        "# -----------------------------------------\n",
        "# third layer: output layer:\n",
        "model.add(Dense(out_size, input_dim = hidden_neurons))  # Nuerons\n",
        "model.add(Activation('softmax')) # Activation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rUwxkVp1ZcO"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'],\n",
        "              optimizer='adam')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgFMQhhr1uye"
      },
      "outputs": [],
      "source": [
        "input_size = 286\n",
        "hidden_neurons = 100\n",
        "out_size = 10\n",
        "\n",
        "## Designing the ANN Structure (with 784 inputs, 10 outputs and 100 neuron in a hidden layer):\n",
        "# -----------------------------------------\n",
        "# first layer: input layer\n",
        "# Input layer does not do any processing, so no need to define the input layer in this problem.\n",
        "\n",
        "# -----------------------------------------\n",
        "# second layer: hidden layer:\n",
        "model.add(Dense(hidden_neurons, input_dim = input_size))  # Nuerons\n",
        "model.add(Activation('sigmoid')) # Activation\n",
        "\n",
        "# -----------------------------------------\n",
        "# third layer: output layer:\n",
        "model.add(Dense(out_size, input_dim = hidden_neurons))  # Nuerons\n",
        "model.add(Activation('softmax')) # Activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V_PyulEY1wfR",
        "outputId": "2ac0fcba-aafc-4b69-c3fb-3fdaa80dd2c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train on 107199 samples, validate on 52801 samples\n",
            "Epoch 1/15\n",
            "107199/107199 [==============================] - 14s 128us/step - loss: 1.8745 - acc: 0.2993 - val_loss: 1.5546 - val_acc: 0.4356\n",
            "Epoch 2/15\n",
            "107199/107199 [==============================] - 13s 123us/step - loss: 1.4078 - acc: 0.4890 - val_loss: 1.2911 - val_acc: 0.5176\n",
            "Epoch 3/15\n",
            "107199/107199 [==============================] - 13s 125us/step - loss: 1.2085 - acc: 0.5971 - val_loss: 1.1347 - val_acc: 0.6379\n",
            "Epoch 4/15\n",
            "107199/107199 [==============================] - 14s 126us/step - loss: 1.0795 - acc: 0.6669 - val_loss: 1.0266 - val_acc: 0.7019\n",
            "Epoch 5/15\n",
            "107199/107199 [==============================] - 14s 126us/step - loss: 0.9872 - acc: 0.7047 - val_loss: 0.9446 - val_acc: 0.7372\n",
            "Epoch 6/15\n",
            "107199/107199 [==============================] - 13s 120us/step - loss: 0.9179 - acc: 0.7236 - val_loss: 0.8872 - val_acc: 0.7365\n",
            "Epoch 7/15\n",
            "107199/107199 [==============================] - 13s 123us/step - loss: 0.8648 - acc: 0.7368 - val_loss: 0.8415 - val_acc: 0.7434\n",
            "Epoch 8/15\n",
            "107199/107199 [==============================] - 13s 124us/step - loss: 0.8230 - acc: 0.7478 - val_loss: 0.8067 - val_acc: 0.7517\n",
            "Epoch 9/15\n",
            "107199/107199 [==============================] - 13s 122us/step - loss: 0.7893 - acc: 0.7570 - val_loss: 0.7728 - val_acc: 0.7698\n",
            "Epoch 10/15\n",
            "107199/107199 [==============================] - 13s 122us/step - loss: 0.7633 - acc: 0.7633 - val_loss: 0.7514 - val_acc: 0.7735\n",
            "Epoch 11/15\n",
            "107199/107199 [==============================] - 13s 125us/step - loss: 0.7405 - acc: 0.7706 - val_loss: 0.7353 - val_acc: 0.7691\n",
            "Epoch 12/15\n",
            "107199/107199 [==============================] - 13s 124us/step - loss: 0.7215 - acc: 0.7784 - val_loss: 0.7149 - val_acc: 0.7859\n",
            "Epoch 13/15\n",
            "107199/107199 [==============================] - 13s 121us/step - loss: 0.7059 - acc: 0.7843 - val_loss: 0.6967 - val_acc: 0.7876\n",
            "Epoch 14/15\n",
            "107199/107199 [==============================] - 13s 117us/step - loss: 0.6916 - acc: 0.7903 - val_loss: 0.6894 - val_acc: 0.7862\n",
            "Epoch 15/15\n",
            "107199/107199 [==============================] - 13s 117us/step - loss: 0.6794 - acc: 0.7936 - val_loss: 0.6768 - val_acc: 0.7775\n"
          ]
        }
      ],
      "source": [
        "#fitted_model = model.fit(X_train, y_train, batch_size=32, epochs=15, verbose=1)\n",
        "fitted_model = model.fit(X_train, y_train, validation_split=0.33, batch_size=32, epochs=15, verbose=1)\n",
        "\n",
        "\n",
        "# batch_size: Integer or None. Number of samples per gradient update. \n",
        "# epochs: Number of iterations over the entire x and y training data. \n",
        "# verbose: 0, 1, or 2. how to see the training progress. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
        "# validation_split: Float between 0 and 1. Fraction of the training data to be used as validation data. \n",
        "# You can add some callbacks to get a view on internal states and statistics of the model during training:\n",
        "# https://keras.io/callbacks/     "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yeW2O_Hc1y4G",
        "outputId": "4e1ba68a-7b72-4c4a-cb63-4d9ff6c075f8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "40000/40000 [==============================] - 1s 27us/step\n",
            "(40000, 10)\n",
            "40000/40000 [==============================] - 1s 31us/step\n",
            "The accuracy is:  0.776675\n"
          ]
        }
      ],
      "source": [
        "# Prediction:\n",
        "y_pridict = model.predict(X_test, verbose=1)\n",
        "print (y_pridict.shape)\n",
        "# Evaluation:\n",
        "score = model.evaluate(X_test, y_test, verbose=1)\n",
        "print('The accuracy is: ', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 573
        },
        "id": "H2-3yPDV1zd4",
        "outputId": "b600b40e-9580-4004-b27e-73252656f5e8"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# summarize history for accuracy\n",
        "plt.plot(fitted_model.history['acc'])\n",
        "plt.plot(fitted_model.history['val_acc'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# summarize history for loss\n",
        "plt.plot(fitted_model.history['loss'])\n",
        "plt.plot(fitted_model.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'val'], loc='upper left')\n",
        "plt.show()\n",
        "\n",
        "# print(fitted_model.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dXo3zyAXEBwB"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzcTLihzEBwB"
      },
      "outputs": [],
      "source": [
        "# Importing libraries and packages:\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn import metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RifVrSkSpKLK",
        "outputId": "633cb985-e029-4696-9377-7338de4626f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[9 6 6 ... 8 7 5]\n"
          ]
        }
      ],
      "source": [
        "le = LabelEncoder()\n",
        "y = le.fit_transform(df_train['target'])\n",
        "X = df_train.drop(columns = ['row_id','target'])\n",
        "print(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8yvI04EpKLL",
        "outputId": "74f546fd-1e6a-4c69-d2c6-991481f823c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(140000, 286)\n",
            "(140000,)\n",
            "(60000, 286)\n",
            "(60000,)\n"
          ]
        }
      ],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2)\n",
        "\n",
        "# print the size of the traning set:\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "# print the size of the testing set:\n",
        "print(X_test.shape)\n",
        "print(y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ku8j85lJEBwB"
      },
      "outputs": [],
      "source": [
        "# \"my_logreg\" is instantiated as an \"object\" of LogisticRegression \"class\". \n",
        "my_logreg = LogisticRegression(max_iter = 2000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "2S8Xl-TsEBwB",
        "outputId": "94490ac1-a4b8-4401-8a2a-c84700c527ed"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "LogisticRegression(max_iter=2000)"
            ]
          },
          "execution_count": 22,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Training ONLY on the training set:\n",
        "my_logreg.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0_rDZwEsEBwB",
        "outputId": "6c06a273-f7e2-46d5-c93b-3e26bea331b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4 4 6 ... 3 5 1]\n"
          ]
        }
      ],
      "source": [
        "# Testing on the testing set:\n",
        "y_predict_lr = my_logreg.predict(X_test)\n",
        "print(y_predict_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEhhwDuQEBwB",
        "outputId": "f2c55e55-409d-45f8-c622-8147f638399c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy is:  0.6578166666666667\n"
          ]
        }
      ],
      "source": [
        "# We can now compare the \"predicted labels\" for the Testing Set with its \"actual labels\" to evaluate the accuracy \n",
        "\n",
        "score_lr = accuracy_score(y_test, y_predict_lr)\n",
        "\n",
        "print('The accuracy is: ', score_lr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0nneaE9tEBwC"
      },
      "outputs": [],
      "source": [
        "# Predicting the Binary Label:\n",
        "y_predict_lr = my_logreg.predict(X_test)\n",
        "\n",
        "# Estimating the probability (likelihood) of Each Label: \n",
        "y_predict_prob_lr = my_logreg.predict_proba(X_test)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pAPC6WQzEBwC",
        "outputId": "68ead132-2d04-4eb2-89c8-575687ec9c14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[4 4 6 ... 3 5 1]\n",
            "[0.04811246 0.02147274 0.01641672 ... 0.09135705 0.01602611 0.66146113]\n"
          ]
        }
      ],
      "source": [
        "# This line prints the \"actual label\" of the testing set:\n",
        "#print(y_test)\n",
        "\n",
        "# This line prints the \"predicted label\" for the testing set:\n",
        "print(y_predict_lr)\n",
        "\n",
        "# This line prints the \"estimated likelihood of both label\" for the testing set:\n",
        "#print(y_predict_prob_lr)\n",
        "\n",
        "# This line prints the \"estimated likelihood of label=1\" for the testing set:\n",
        "print(y_predict_prob_lr[:,1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aTKFUjAcEBwC",
        "outputId": "05089da1-7651-4b31-bb2d-cacfde862306"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[0.         0.         0.         ... 0.99992584 0.99998146 1.        ]\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "[0.00000000e+00 3.29706561e-04 8.24266403e-04 ... 1.00000000e+00\n",
            " 1.00000000e+00 1.00000000e+00]\n"
          ]
        }
      ],
      "source": [
        "fpr, tpr, thresholds = metrics.roc_curve(y_test, y_predict_prob_lr[:,1], pos_label=1)\n",
        "\n",
        "print(fpr)\n",
        "print(\"\\n\\n\\n\")\n",
        "print(tpr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGFKq3MREBwC",
        "outputId": "4f4ed9d6-5d16-43e1-9510-565cc187ffdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.9263825506235039\n"
          ]
        }
      ],
      "source": [
        "# AUC:\n",
        "AUC = metrics.auc(fpr, tpr)\n",
        "print(AUC)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqDUYOdsEBwC",
        "outputId": "c24c6fd9-622c-448d-e1d5-1e1ae853bda2"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "# The following line will tell Jupyter Notebook to keep the figures inside the explorer page \n",
        "# rather than openng a new figure window:\n",
        "%matplotlib inline\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "# Roc Curve:\n",
        "plt.plot(fpr, tpr, color='red', lw=2, \n",
        "         label='ROC Curve (area = %0.2f)' % AUC)\n",
        "\n",
        "# Random Guess line:\n",
        "plt.plot([0, 1], [0, 1], color='blue', lw=1, linestyle='--')\n",
        "\n",
        "# Defining The Range of X-Axis and Y-Axis:\n",
        "plt.xlim([-0.005, 1.005])\n",
        "plt.ylim([0.0, 1.01])\n",
        "\n",
        "# Labels, Title, Legend:\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver operating characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6flJiIrpKLP"
      },
      "source": [
        "# Logistic Regression with Cross Validation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8xBBhAApKLQ",
        "outputId": "f2e64254-d873-4408-941a-2becf3f77d05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The accuracy is:  0.6757500000000001\n"
          ]
        }
      ],
      "source": [
        "# Applying 10-fold cross validation with \"Logistic regression\" classifier:\n",
        "\n",
        "#In the following line, \"my_cross_lr\" is instantiated as an \"object\" of LogisticRegression \"class\".\n",
        "my_cross_lr = LogisticRegression(max_iter = 2000)\n",
        "\n",
        "# function cross_val_score performs Cross Validation:\n",
        "accuracy_list = cross_val_score(my_cross_lr, X, y, cv = 10, scoring = 'accuracy')\n",
        "\n",
        "accuracy_cv = accuracy_list.mean()\n",
        "\n",
        "print('The accuracy is: ',accuracy_cv)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLTWJFNjpKLQ"
      },
      "source": [
        "We can see here that the accuracy without cross validation was 65.8%. While the accuracy with cross validation was 67.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTN2aSvGpKLR"
      },
      "source": [
        "## In summary the Keras ANN model with an AUC of 77% was the worst model. While the Logistic Regression model with an AUC of 93% did better. But, it was the SciKitLearn ANN model with an AUC of 99% that was the best model."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}